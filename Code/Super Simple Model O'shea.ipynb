{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"references\\Subject_info.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Research Group</th>\n",
       "      <th>APOE A1</th>\n",
       "      <th>APOE A2</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>File_Path</th>\n",
       "      <th>File_Path_desktop</th>\n",
       "      <th>PATH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002_S_0295</td>\n",
       "      <td>M</td>\n",
       "      <td>73.0</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_0295_f...</td>\n",
       "      <td>data\\processed\\002_S_0295_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_S_0413</td>\n",
       "      <td>F</td>\n",
       "      <td>57.6</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>81.5</td>\n",
       "      <td>test</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_0413_f...</td>\n",
       "      <td>data\\processed\\002_S_0413_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002_S_0685</td>\n",
       "      <td>F</td>\n",
       "      <td>68.9</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>95.8</td>\n",
       "      <td>test</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_0685_f...</td>\n",
       "      <td>data\\processed\\002_S_0685_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002_S_0729</td>\n",
       "      <td>F</td>\n",
       "      <td>65.8</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>71.3</td>\n",
       "      <td>validation</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_0729_f...</td>\n",
       "      <td>data\\processed\\002_S_0729_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002_S_1155</td>\n",
       "      <td>M</td>\n",
       "      <td>64.9</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_1155_f...</td>\n",
       "      <td>data\\processed\\002_S_1155_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>941_S_4377</td>\n",
       "      <td>F</td>\n",
       "      <td>121.6</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>test</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_4377_f...</td>\n",
       "      <td>data\\processed\\941_S_4377_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>941_S_4420</td>\n",
       "      <td>M</td>\n",
       "      <td>79.4</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>81.5</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_4420_f...</td>\n",
       "      <td>data\\processed\\941_S_4420_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>941_S_4764</td>\n",
       "      <td>F</td>\n",
       "      <td>77.6</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>82.8</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_4764_f...</td>\n",
       "      <td>data\\processed\\941_S_4764_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>941_S_5124</td>\n",
       "      <td>F</td>\n",
       "      <td>78.9</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>76.8</td>\n",
       "      <td>test</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_5124_f...</td>\n",
       "      <td>data\\processed\\941_S_5124_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>941_S_5193</td>\n",
       "      <td>F</td>\n",
       "      <td>88.9</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>72.6</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_5193_f...</td>\n",
       "      <td>data\\processed\\941_S_5193_fused.nii</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Subject Sex  Weight Research Group  APOE A1  APOE A2   Age  \\\n",
       "0    002_S_0295   M    73.0             CN      3.0      4.0  90.0   \n",
       "1    002_S_0413   F    57.6             CN      3.0      3.0  81.5   \n",
       "2    002_S_0685   F    68.9             CN      3.0      3.0  95.8   \n",
       "3    002_S_0729   F    65.8            MCI      3.0      4.0  71.3   \n",
       "4    002_S_1155   M    64.9            MCI      3.0      3.0  64.0   \n",
       "..          ...  ..     ...            ...      ...      ...   ...   \n",
       "173  941_S_4377   F   121.6            MCI      3.0      4.0  69.5   \n",
       "174  941_S_4420   M    79.4            MCI      3.0      3.0  81.5   \n",
       "175  941_S_4764   F    77.6            MCI      3.0      3.0  82.8   \n",
       "176  941_S_5124   F    78.9             CN      3.0      3.0  76.8   \n",
       "177  941_S_5193   F    88.9             CN      3.0      3.0  72.6   \n",
       "\n",
       "    dataset_split                                          File_Path  \\\n",
       "0           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "1            test  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "2            test  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "3      validation  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "4           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "..            ...                                                ...   \n",
       "173          test  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "174         train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "175         train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "176          test  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "177         train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "\n",
       "                                     File_Path_desktop  \\\n",
       "0    D:\\Data\\Preprocessed\\Fused Images\\002_S_0295_f...   \n",
       "1    D:\\Data\\Preprocessed\\Fused Images\\002_S_0413_f...   \n",
       "2    D:\\Data\\Preprocessed\\Fused Images\\002_S_0685_f...   \n",
       "3    D:\\Data\\Preprocessed\\Fused Images\\002_S_0729_f...   \n",
       "4    D:\\Data\\Preprocessed\\Fused Images\\002_S_1155_f...   \n",
       "..                                                 ...   \n",
       "173  D:\\Data\\Preprocessed\\Fused Images\\941_S_4377_f...   \n",
       "174  D:\\Data\\Preprocessed\\Fused Images\\941_S_4420_f...   \n",
       "175  D:\\Data\\Preprocessed\\Fused Images\\941_S_4764_f...   \n",
       "176  D:\\Data\\Preprocessed\\Fused Images\\941_S_5124_f...   \n",
       "177  D:\\Data\\Preprocessed\\Fused Images\\941_S_5193_f...   \n",
       "\n",
       "                                    PATH  \n",
       "0    data\\processed\\002_S_0295_fused.nii  \n",
       "1    data\\processed\\002_S_0413_fused.nii  \n",
       "2    data\\processed\\002_S_0685_fused.nii  \n",
       "3    data\\processed\\002_S_0729_fused.nii  \n",
       "4    data\\processed\\002_S_1155_fused.nii  \n",
       "..                                   ...  \n",
       "173  data\\processed\\941_S_4377_fused.nii  \n",
       "174  data\\processed\\941_S_4420_fused.nii  \n",
       "175  data\\processed\\941_S_4764_fused.nii  \n",
       "176  data\\processed\\941_S_5124_fused.nii  \n",
       "177  data\\processed\\941_S_5193_fused.nii  \n",
       "\n",
       "[178 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "def preprocess_labels(df):\n",
    "    # Mapping similar categories to a single category\n",
    "    label_mapping = {\n",
    "        'EMCI': 'MCI',\n",
    "        'LMCI': 'MCI',\n",
    "        'SMC': 'CN'  # If you want SMC to be considered as CN, include this; remove if not\n",
    "    }\n",
    "    df['Research Group'] = df['Research Group'].replace(label_mapping)\n",
    "    return df\n",
    "\n",
    "preprocess_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df = df[df['Research Group'] != 'AD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to hold the balanced data\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each dataset split\n",
    "for split in df['dataset_split'].unique():\n",
    "    # Filter the DataFrame for the current split\n",
    "    split_df = df[df['dataset_split'] == split]\n",
    "    \n",
    "    # Find the minimum number of rows for any Research Group within this split\n",
    "    min_size = split_df['Research Group'].value_counts().min()\n",
    "    \n",
    "    # Sample from each group to match the minimum size\n",
    "    sampled_groups = [group_df.sample(n=min_size, random_state=1) \n",
    "                      for name, group_df in split_df.groupby('Research Group')]\n",
    "    \n",
    "    # Concatenate the sampled groups into the balanced DataFrame\n",
    "    balanced_df = pd.concat([balanced_df, *sampled_groups], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Research Group\n",
       "CN     64\n",
       "MCI    64\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = balanced_df\n",
    "df['Research Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Research Group</th>\n",
       "      <th>APOE A1</th>\n",
       "      <th>APOE A2</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>File_Path</th>\n",
       "      <th>File_Path_desktop</th>\n",
       "      <th>PATH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002_S_1280</td>\n",
       "      <td>F</td>\n",
       "      <td>89.4</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>75.1</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_1280_f...</td>\n",
       "      <td>data\\processed\\002_S_1280_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128_S_0863</td>\n",
       "      <td>M</td>\n",
       "      <td>92.1</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>79.3</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\128_S_0863_f...</td>\n",
       "      <td>data\\processed\\128_S_0863_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002_S_4213</td>\n",
       "      <td>F</td>\n",
       "      <td>80.0</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>78.1</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_4213_f...</td>\n",
       "      <td>data\\processed\\002_S_4213_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>032_S_5289</td>\n",
       "      <td>F</td>\n",
       "      <td>79.4</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>59.8</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\032_S_5289_f...</td>\n",
       "      <td>data\\processed\\032_S_5289_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>053_S_5296</td>\n",
       "      <td>M</td>\n",
       "      <td>76.0</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>69.3</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\053_S_5296_f...</td>\n",
       "      <td>data\\processed\\053_S_5296_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>002_S_4447</td>\n",
       "      <td>F</td>\n",
       "      <td>68.5</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69.7</td>\n",
       "      <td>validation</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_4447_f...</td>\n",
       "      <td>data\\processed\\002_S_4447_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>022_S_2167</td>\n",
       "      <td>M</td>\n",
       "      <td>71.4</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>83.2</td>\n",
       "      <td>validation</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\022_S_2167_f...</td>\n",
       "      <td>data\\processed\\022_S_2167_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>022_S_4805</td>\n",
       "      <td>F</td>\n",
       "      <td>43.5</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>72.2</td>\n",
       "      <td>validation</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\022_S_4805_f...</td>\n",
       "      <td>data\\processed\\022_S_4805_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>002_S_4251</td>\n",
       "      <td>M</td>\n",
       "      <td>78.0</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>validation</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_4251_f...</td>\n",
       "      <td>data\\processed\\002_S_4251_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>128_S_4553</td>\n",
       "      <td>F</td>\n",
       "      <td>54.9</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68.9</td>\n",
       "      <td>validation</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\128_S_4553_f...</td>\n",
       "      <td>data\\processed\\128_S_4553_fused.nii</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Subject Sex  Weight Research Group  APOE A1  APOE A2   Age  \\\n",
       "0    002_S_1280   F    89.4             CN      3.0      4.0  75.1   \n",
       "1    128_S_0863   M    92.1             CN      3.0      3.0  79.3   \n",
       "2    002_S_4213   F    80.0             CN      3.0      3.0  78.1   \n",
       "3    032_S_5289   F    79.4             CN      3.0      4.0  59.8   \n",
       "4    053_S_5296   M    76.0             CN      3.0      3.0  69.3   \n",
       "..          ...  ..     ...            ...      ...      ...   ...   \n",
       "123  002_S_4447   F    68.5            MCI      3.0      4.0  69.7   \n",
       "124  022_S_2167   M    71.4            MCI      3.0      3.0  83.2   \n",
       "125  022_S_4805   F    43.5            MCI      3.0      4.0  72.2   \n",
       "126  002_S_4251   M    78.0            MCI      3.0      3.0  72.0   \n",
       "127  128_S_4553   F    54.9            MCI      3.0      3.0  68.9   \n",
       "\n",
       "    dataset_split                                          File_Path  \\\n",
       "0           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "1           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "2           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "3           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "4           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "..            ...                                                ...   \n",
       "123    validation  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "124    validation  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "125    validation  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "126    validation  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "127    validation  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "\n",
       "                                     File_Path_desktop  \\\n",
       "0    D:\\Data\\Preprocessed\\Fused Images\\002_S_1280_f...   \n",
       "1    D:\\Data\\Preprocessed\\Fused Images\\128_S_0863_f...   \n",
       "2    D:\\Data\\Preprocessed\\Fused Images\\002_S_4213_f...   \n",
       "3    D:\\Data\\Preprocessed\\Fused Images\\032_S_5289_f...   \n",
       "4    D:\\Data\\Preprocessed\\Fused Images\\053_S_5296_f...   \n",
       "..                                                 ...   \n",
       "123  D:\\Data\\Preprocessed\\Fused Images\\002_S_4447_f...   \n",
       "124  D:\\Data\\Preprocessed\\Fused Images\\022_S_2167_f...   \n",
       "125  D:\\Data\\Preprocessed\\Fused Images\\022_S_4805_f...   \n",
       "126  D:\\Data\\Preprocessed\\Fused Images\\002_S_4251_f...   \n",
       "127  D:\\Data\\Preprocessed\\Fused Images\\128_S_4553_f...   \n",
       "\n",
       "                                    PATH  \n",
       "0    data\\processed\\002_S_1280_fused.nii  \n",
       "1    data\\processed\\128_S_0863_fused.nii  \n",
       "2    data\\processed\\002_S_4213_fused.nii  \n",
       "3    data\\processed\\032_S_5289_fused.nii  \n",
       "4    data\\processed\\053_S_5296_fused.nii  \n",
       "..                                   ...  \n",
       "123  data\\processed\\002_S_4447_fused.nii  \n",
       "124  data\\processed\\022_S_2167_fused.nii  \n",
       "125  data\\processed\\022_S_4805_fused.nii  \n",
       "126  data\\processed\\002_S_4251_fused.nii  \n",
       "127  data\\processed\\128_S_4553_fused.nii  \n",
       "\n",
       "[128 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame loaded with the 'Research Group' column available\n",
    "label_categories = pd.Categorical(df['Research Group'])\n",
    "label_mapping = {code: category for code, category in enumerate(label_categories.categories)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NiiDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object.\n",
    "        :param df: DataFrame containing file paths, labels, and subject IDs.\n",
    "        :param transform: A function or a series of transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        self.paths = df['PATH'].tolist()  # Paths to .nii files\n",
    "        self.labels = pd.Categorical(df['Research Group'], categories=pd.Categorical(df['Research Group']).categories).codes\n",
    "        self.subjects = df['Subject'].tolist()  # Subject identifiers\n",
    "        self.transform = transform  # Transformation function(s)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        image = self.load_nii(path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        subject = self.subjects[idx]\n",
    "        return image, label, path, subject\n",
    "\n",
    "    def load_nii(self, path):\n",
    "        \"\"\"\n",
    "        Load a NIfTI file and normalize its intensity.\n",
    "        \"\"\"\n",
    "        image = nib.load(path).get_fdata(dtype=np.float32)\n",
    "        image = self.normalize_intensity(image)\n",
    "        image = np.expand_dims(image, axis=0)  # Add a channel dimension\n",
    "        return image\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_intensity(image):\n",
    "        \"\"\"\n",
    "        Normalize the image data to zero mean and unit variance.\n",
    "        \"\"\"\n",
    "        mean_intensity = np.mean(image)\n",
    "        std_intensity = np.std(image)\n",
    "        normalized_image = (image - mean_intensity) / std_intensity\n",
    "        return normalized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def load_datasets(df):\n",
    "    train_df = df[df['dataset_split'] == 'train']\n",
    "    val_df = df[df['dataset_split'] == 'validation']\n",
    "    test_df = df[df['dataset_split'] == 'test']\n",
    "    \n",
    "    train_dataset = NiiDataset(train_df)\n",
    "    val_dataset = NiiDataset(val_df)\n",
    "    test_dataset = NiiDataset(test_df)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=4):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Simple3DCNNAlt1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple3DCNNAlt1, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 32, kernel_size=3, padding=1)  # Output will be (32, 193, 229, 193)\n",
    "        self.pool = nn.MaxPool3d(2, stride=2)  # Output will be (32, 96, 114, 96)\n",
    "        self.fc1 = nn.Linear(32 * 96 * 114 * 96, 128)  # Flatten output\n",
    "        self.fc2 = nn.Linear(128, 2)  # Output size for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 32 * 96 * 114 * 96)  # Flatten the output for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, label_mapping, num_epochs=10, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_results = []\n",
    "    train_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        epoch_losses = []\n",
    "        for images, labels, paths, subjects in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted_indices == labels).sum().item()\n",
    "            predicted_labels = [label_mapping[code] for code in predicted_indices.cpu().numpy()]\n",
    "\n",
    "            for label, pred, path, subject in zip(labels.cpu().numpy(), predicted_labels, paths, subjects):\n",
    "                train_results.append({\n",
    "                    'Subject': subject,\n",
    "                    'Path': path,\n",
    "                    'Actual Label': label_mapping[label.item()],\n",
    "                    'Prediction': pred,\n",
    "                    'Type': 'Train'\n",
    "                })\n",
    "        \n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        print(f\"Average loss for Epoch {epoch+1}: {avg_loss:.4f} - Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    return train_results, train_accuracies\n",
    "\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, criterion, label_mapping, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    validation_results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths, subjects in tqdm(val_loader, desc='Validation'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted_indices == labels).sum().item()\n",
    "            predicted_labels = [label_mapping[code] for code in predicted_indices.cpu().numpy()]\n",
    "\n",
    "            for label, pred, path, subject in zip(labels.cpu().numpy(), predicted_labels, paths, subjects):\n",
    "                validation_results.append({\n",
    "                    'Subject': subject,\n",
    "                    'Path': path,\n",
    "                    'Actual Label': label_mapping[label.item()],\n",
    "                    'Prediction': pred,\n",
    "                    'Type': 'Validation'\n",
    "                })\n",
    "    accuracy = 100 * correct / total\n",
    "    return validation_results, accuracy\n",
    "\n",
    "def test_model(model, test_loader, label_mapping, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths, subjects in tqdm(test_loader, desc='Testing'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted_indices == labels).sum().item()\n",
    "            predicted_labels = [label_mapping[code] for code in predicted_indices.cpu().numpy()]\n",
    "\n",
    "            for label, pred, path, subject in zip(labels.cpu().numpy(), predicted_labels, paths, subjects):\n",
    "                test_results.append({\n",
    "                    'Subject': subject,\n",
    "                    'Path': path,\n",
    "                    'Actual Label': label_mapping[label.item()],\n",
    "                    'Prediction': pred,\n",
    "                    'Type': 'Test'\n",
    "                })\n",
    "    accuracy = 100 * correct / total\n",
    "    return test_results, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "def run_experiment(df, config):\n",
    "    \"\"\"Run the experiment with the given configuration on the preprocessed DataFrame.\"\"\"\n",
    "    train_dataset, val_dataset, test_dataset = load_datasets(df)\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Simple3DCNNAlt1().to(device)  # Use the resnet18_3d function to initialize the model\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.get('learning_rate', 0.001))  # Ensure learning rate is in config or use default\n",
    "\n",
    "    # Training and validation\n",
    "    train_results, train_accuracies = train_model(model, train_loader, criterion, optimizer, label_mapping, config['num_epochs'], device)\n",
    "    validate_results, val_accuracy = validate_model(model, val_loader, criterion, label_mapping, device)\n",
    "    test_results, test_accuracy = test_model(model, test_loader, label_mapping, device)\n",
    "    \n",
    "    # Save detailed results to Excel\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = os.path.join('reports', f'{formatted_time}_Experiment.xlsx')\n",
    "    \n",
    "    summary_data = {\n",
    "        'Phase': ['Training', 'Validation', 'Testing'],\n",
    "        'Accuracy': [train_accuracies[-1], val_accuracy, test_accuracy]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    all_results = pd.DataFrame(train_results + validate_results + test_results)\n",
    "    config_df = pd.DataFrame([config])\n",
    "    \n",
    "    with pd.ExcelWriter(filename) as writer:\n",
    "        config_df.to_excel(writer, sheet_name='Configuration')\n",
    "        all_results.to_excel(writer, sheet_name='Results')\n",
    "        summary_df.to_excel(writer, sheet_name='Summary')\n",
    "\n",
    "    # Append a summary of this experiment to the cumulative RESULTS.xlsx file\n",
    "    results_file = os.path.join('reports', 'RESULTS.xlsx')\n",
    "    experiment_summary = {**config, **{'Training Accuracy': train_accuracies[-1], 'Validation Accuracy': val_accuracy, 'Test Accuracy': test_accuracy, 'DATETIME': formatted_time}}\n",
    "    summary_row = pd.DataFrame([experiment_summary])\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        with pd.ExcelWriter(results_file, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "            summary_row.to_excel(writer, startrow=writer.sheets['Sheet1'].max_row, index=False, header=False)\n",
    "    else:\n",
    "        summary_row.to_excel(results_file, index=False)\n",
    "\n",
    "    return filename, train_accuracies[-1], val_accuracy, test_accuracy\n",
    "\n",
    "# Update config to reflect necessary parameters for ResNet3D\n",
    "config = {\n",
    "    'num_classes': 2,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 4,\n",
    "    'Description': 'Super Simple'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/19 [00:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.07 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 20\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(df, config)\u001b[0m\n\u001b[0;32m     17\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.001\u001b[39m))  \u001b[38;5;66;03m# Ensure learning rate is in config or use default\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Training and validation\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m train_results, train_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m validate_results, val_accuracy \u001b[38;5;241m=\u001b[39m validate_model(model, val_loader, criterion, label_mapping, device)\n\u001b[0;32m     22\u001b[0m test_results, test_accuracy \u001b[38;5;241m=\u001b[39m test_model(model, test_loader, label_mapping, device)\n",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, label_mapping, num_epochs, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m, in \u001b[0;36mSimple3DCNNAlt1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m96\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m114\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m96\u001b[39m)  \u001b[38;5;66;03m# Flatten the output for the fully connected layer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\torch\\nn\\functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.07 GiB. GPU "
     ]
    }
   ],
   "source": [
    "run_experiment(df, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_and_validate_model(model, train_loader, val_loader, criterion, optimizer, label_mapping, num_epochs=10, patience=3, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_losses = []\n",
    "        model.train()\n",
    "        for images, labels, paths, subjects in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        val_loss, val_accuracy = validate_model(model, val_loader, criterion, label_mapping, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train loss: {avg_train_loss:.4f}, Validation loss: {val_loss:.4f}, Validation accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Early stopping mechanism\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve_epoch = 0\n",
    "        else:\n",
    "            no_improve_epoch += 1\n",
    "            if no_improve_epoch >= patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "def validate_model(model, val_loader, criterion, label_mapping, device='cpu'):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths, subjects in tqdm(val_loader, desc='Validation'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "            correct += (predicted_indices == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_val_loss, accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
