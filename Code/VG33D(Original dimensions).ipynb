{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Specify the full path to the module file\n",
    "module_path = 'D:\\\\Github Folder\\\\MasterThesis\\\\Code\\\\FUNCTIONS.py'\n",
    "\n",
    "# Load the module\n",
    "spec = importlib.util.spec_from_file_location(\"FUNCTIONS\", module_path)\n",
    "functions = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(functions)\n",
    "\n",
    "# Now you can use the functions as if you had imported them\n",
    "load_datasets = functions.load_datasets\n",
    "create_dataloaders = functions.create_dataloaders\n",
    "train_and_validate = functions.train_and_validate\n",
    "test_model = functions.test_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"references\\NEW_COMBINED_FINAL_Subject_info.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame loaded with the 'Research Group' column available\n",
    "label_categories = pd.Categorical(df['Research Group'])\n",
    "label_mapping = {code: category for code, category in enumerate(label_categories.categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VGG3D(nn.Module):\n",
    "    def __init__(self, num_classes=1, input_shape=(1, 193, 229, 193)):\n",
    "        super(VGG3D, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=input_shape[0], out_channels=8, kernel_size=(3,3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=8, out_channels=8, kernel_size=(3,3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2,2,2), stride=3)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=8, out_channels=16, kernel_size=(3,3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=16, out_channels=16, kernel_size=(3,3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2,2,2), stride=3)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=16, out_channels=32, kernel_size=(3,3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=32, out_channels=32, kernel_size=(3,3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=32, out_channels=32, kernel_size=(3,3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2,2,2), stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=32, out_channels=64, kernel_size=(3,3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(3,3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(3,3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2,2,2), stride=2)\n",
    "        )\n",
    "        \n",
    "        # This will be updated after running the model to find the correct flattened size\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(9600, 128),  # This number will be updated\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if(num_classes==2):\n",
    "            self.out = nn.Linear(64, 2)\n",
    "            self.out_act = nn.Sigmoid()\n",
    "        else:\n",
    "            self.out = nn.Linear(64, num_classes)\n",
    "            self.out_act = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, drop_prob=0.8):\n",
    "       # print(\"Input shape:\", x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print(\"After Conv1:\", x.shape)\n",
    "        x = self.conv2(x)\n",
    "       # print(\"After Conv2:\", x.shape)\n",
    "        x = self.conv3(x)\n",
    "       # print(\"After Conv3:\", x.shape)\n",
    "        x = self.conv4(x)\n",
    "       # print(\"After Conv4:\", x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "       #print(\"Shape before FC1:\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.Dropout(drop_prob)(x)\n",
    "        x = self.fc2(x)\n",
    "        prob = self.out(x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_optimizer(model, config):\n",
    "    \"\"\"Select optimizer based on configuration.\"\"\"\n",
    "    lr = config.get('lr', 0.001)  # Default learning rate\n",
    "    weight_decay = config.get('weight_decay', 0)  # Default weight decay\n",
    "    \n",
    "    if config['optimizer'] == 'Adam':\n",
    "        return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        momentum = config.get('momentum', 0.9)  # Default momentum for SGD\n",
    "        return optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "def select_criterion(config):\n",
    "    \"\"\"Select loss criterion based on configuration.\"\"\"\n",
    "    if config['loss_criterion'] == 'Cross-Entropy':\n",
    "        return nn.CrossEntropyLoss()\n",
    "    elif config['loss_criterion'] == 'BCEWithLogits':\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss criterion\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_experiment(df, config):\n",
    "    train_dataset, val_dataset, test_dataset = load_datasets(df, image_type = config['image_type'], sample_size = config['sample_size'], loss_type = config['loss_criterion'])\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    model = VGG3D().to(device)\n",
    "    \n",
    "    criterion = select_criterion(config)\n",
    "    optimizer = select_optimizer(model, config)\n",
    "\n",
    "\n",
    "    # Training and validation\n",
    "    train_accuracies, val_accuracies, val_losses, results_df = train_and_validate(model, train_loader, val_loader, criterion, optimizer, config['num_epochs'], config['patience'], device, config['loss_criterion'])\n",
    "    test_results, test_accuracy = test_model(model, test_loader, label_mapping, device, loss_type = config['loss_criterion'])\n",
    "    \n",
    "    # Save detailed results to Excel\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    report_filename = os.path.join('reports', f'{formatted_time}_Experiment.xlsx')\n",
    "    onnx_filename = os.path.join('models', f'{formatted_time}_Model.')\n",
    "          # Save the model to ONNX\n",
    "\n",
    "\n",
    "    \n",
    "    summary_data = {\n",
    "        'Phase': ['Training', 'Validation', 'Testing'],\n",
    "        'Accuracy': [train_accuracies[-1], val_accuracies[-1], test_accuracy]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    all_results = pd.DataFrame(test_results)\n",
    "    config_df = pd.DataFrame([config])\n",
    "    \n",
    "    with pd.ExcelWriter(report_filename) as writer:\n",
    "        config_df.to_excel(writer, sheet_name='Configuration')\n",
    "        all_results.to_excel(writer, sheet_name='Results')\n",
    "        summary_df.to_excel(writer, sheet_name='Summary')\n",
    "        results_df.to_excel(writer, sheet_name='Training_Results')\n",
    "\n",
    "# Append a summary of this experiment to the cumulative RESULTS.xlsx file\n",
    "    results_file = os.path.join('reports', 'RESULTS.xlsx')\n",
    "    experiment_summary = {**config, **{'Training Accuracy': train_accuracies[-1], 'Validation Accuracy': val_accuracies[-1], 'Test Accuracy': test_accuracy, 'DATETIME': formatted_time}}\n",
    "    summary_row = pd.DataFrame([experiment_summary])\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        with pd.ExcelWriter(results_file, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "            existing_df = pd.read_excel(results_file)\n",
    "            combined_df = pd.concat([existing_df, summary_row], ignore_index=True)\n",
    "            combined_df = combined_df.reindex(columns=(existing_df.columns.tolist() + [col for col in summary_row.columns if col not in existing_df.columns]))\n",
    "            combined_df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "    else:\n",
    "        summary_row.to_excel(results_file, index=False)\n",
    "\n",
    "    dummy_input = torch.randn(4, 1, 193, 229, 193, device=device)  # Adjust size according to your model's input\n",
    "    torch.onnx.export(model, dummy_input, onnx_filename, export_params=True)\n",
    "\n",
    "    return report_filename, train_accuracies[-1], val_accuracies[-1], test_accuracy\n",
    "\n",
    "\n",
    "# Example configuration and use case\n",
    "config = {\n",
    "    'optimizer': 'Adam',\n",
    "    'lr' : 0.01,\n",
    "    'loss_criterion': 'BCEWithLogits',\n",
    "    'num_epochs': 100,\n",
    "    'batch_size': 4,\n",
    "    'patience': 20,\n",
    "    'Description' : 'REAL DIMENSIONS',\n",
    "    'image_type' : 'Resampled Images_fused',\n",
    "    'weight_decay' : 0,\n",
    "    'momentum' : 0.7,\n",
    "    'sample_size' : None\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on image type: Fused Images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train: 100%|██████████| 78/78 [19:17<00:00, 14.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.7337 - Train Accuracy: 49.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Validate: 100%|██████████| 11/11 [01:50<00:00, 10.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Loss: 0.7122 - Validation Accuracy: 59.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 - Train: 100%|██████████| 78/78 [20:06<00:00, 15.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.7280 - Train Accuracy: 46.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 - Validate: 100%|██████████| 11/11 [01:49<00:00,  9.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Loss: 0.6909 - Validation Accuracy: 47.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 - Train: 100%|██████████| 78/78 [19:51<00:00, 15.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.7197 - Train Accuracy: 47.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Validation Loss: 0.6651 - Validation Accuracy: 70.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 - Train: 100%|██████████| 78/78 [18:12<00:00, 14.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.7011 - Train Accuracy: 52.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Validation Loss: 0.6969 - Validation Accuracy: 47.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.7092 - Train Accuracy: 47.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Validation Loss: 0.6707 - Validation Accuracy: 63.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.7054 - Train Accuracy: 48.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Validation Loss: 0.7686 - Validation Accuracy: 47.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.7019 - Train Accuracy: 47.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Validation Loss: 0.7290 - Validation Accuracy: 47.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.6996 - Train Accuracy: 45.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Validation Loss: 0.6900 - Validation Accuracy: 56.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.7028 - Train Accuracy: 46.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Validation Loss: 0.6901 - Validation Accuracy: 47.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.6985 - Train Accuracy: 48.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Validation Loss: 0.6769 - Validation Accuracy: 52.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.6987 - Train Accuracy: 48.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Validation Loss: 0.6940 - Validation Accuracy: 43.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.6941 - Train Accuracy: 49.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Validation Loss: 0.6841 - Validation Accuracy: 52.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 0.6978 - Train Accuracy: 50.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Validation Loss: 0.6845 - Validation Accuracy: 52.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 0.6989 - Train Accuracy: 49.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Validation Loss: 0.6998 - Validation Accuracy: 47.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.6950 - Train Accuracy: 49.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Validation Loss: 0.6800 - Validation Accuracy: 63.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.6974 - Train Accuracy: 49.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Validation Loss: 0.6950 - Validation Accuracy: 54.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.6956 - Train Accuracy: 49.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Validation Loss: 0.6907 - Validation Accuracy: 59.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.6957 - Train Accuracy: 45.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Validation Loss: 0.6852 - Validation Accuracy: 56.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 0.6950 - Train Accuracy: 45.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Validation Loss: 0.6869 - Validation Accuracy: 63.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 0.6969 - Train Accuracy: 48.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Validation Loss: 0.6840 - Validation Accuracy: 59.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 0.6971 - Train Accuracy: 44.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Validation Loss: 0.6938 - Validation Accuracy: 40.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 - Train: 100%|██████████| 78/78 [18:09<00:00, 13.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss: 0.6941 - Train Accuracy: 46.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 - Validate: 100%|██████████| 11/11 [01:37<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Validation Loss: 0.6921 - Validation Accuracy: 54.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 - Train: 100%|██████████| 78/78 [18:30<00:00, 14.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss: 0.6951 - Train Accuracy: 41.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 - Validate: 100%|██████████| 11/11 [01:38<00:00,  9.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Validation Loss: 0.6925 - Validation Accuracy: 47.73%\n",
      "Early stopping triggered after 23 epochs due to no improvement in validation loss or accuracy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 22/22 [03:17<00:00,  9.00s/it]\n",
      "C:\\Users\\Micha\\AppData\\Local\\Temp\\ipykernel_46428\\2820527412.py:60: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat([existing_df, summary_row], ignore_index=True)\n",
      "c:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:1515: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'dropout' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on image type: Co-registered PET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train:  35%|███▍      | 27/78 [06:39<12:35, 14.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m image_type\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorking on image type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[1;32mIn[32], line 26\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(df, config)\u001b[0m\n\u001b[0;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m select_optimizer(model, config)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Training and validation\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m train_accuracies, val_accuracies, val_losses, results_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss_criterion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m test_results, test_accuracy \u001b[38;5;241m=\u001b[39m test_model(model, test_loader, label_mapping, device, loss_type \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_criterion\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Save detailed results to Excel\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Github Folder\\MasterThesis\\Code\\FUNCTIONS.py:167\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device, loss_type)\u001b[0m\n\u001b[0;32m    165\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    166\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 167\u001b[0m train_epoch_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    169\u001b[0m train_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m compute_accuracy(outputs, labels, loss_type)\n\u001b[0;32m    170\u001b[0m train_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mnumel()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "  #column_mapping = {\n",
    "  #          'Co-registered PET': 'Co-registered PET',\n",
    "   #         'Fused Images': 'Fused Images',\n",
    "    #        'Masked PET': 'Masked PET',\n",
    "     #       'Spatial Normalization': 'Spatial Normalization',\n",
    "      #      'Resampled Images(Co-registered PET)': 'Resampled Images(Co-registered PET)',\n",
    "       #     'Resampled Images(Masked PET)': 'Resampled Images(Masked PET)',\n",
    "        #    'Resampled Images(Spatial Normalization)': 'Resampled Images(Spatial Normalization)',\n",
    "         #   'Resampled Images_fused': 'Resampled Images_fused'\n",
    "        #}\n",
    "\n",
    "image_types = ['Fused Images', 'Co-registered PET', 'Masked PET', 'Spatial Normalization']\n",
    "results = []\n",
    "for image_type in image_types:\n",
    "    config['image_type'] = image_type\n",
    "    print(f\"Working on image type: {image_type}\")\n",
    "    result = run_experiment(df, config)\n",
    "    results.append(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
