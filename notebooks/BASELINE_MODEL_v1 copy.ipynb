{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"references\\Subject_info.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Research Group</th>\n",
       "      <th>APOE A1</th>\n",
       "      <th>APOE A2</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>File_Path</th>\n",
       "      <th>File_Path_desktop</th>\n",
       "      <th>PATH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002_S_0295</td>\n",
       "      <td>M</td>\n",
       "      <td>73.0</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_0295_f...</td>\n",
       "      <td>data\\processed\\002_S_0295_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_S_0413</td>\n",
       "      <td>F</td>\n",
       "      <td>57.6</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>81.5</td>\n",
       "      <td>test</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_0413_f...</td>\n",
       "      <td>data\\processed\\002_S_0413_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002_S_0685</td>\n",
       "      <td>F</td>\n",
       "      <td>68.9</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>95.8</td>\n",
       "      <td>test</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_0685_f...</td>\n",
       "      <td>data\\processed\\002_S_0685_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002_S_0729</td>\n",
       "      <td>F</td>\n",
       "      <td>65.8</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>71.3</td>\n",
       "      <td>validation</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_0729_f...</td>\n",
       "      <td>data\\processed\\002_S_0729_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002_S_1155</td>\n",
       "      <td>M</td>\n",
       "      <td>64.9</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_1155_f...</td>\n",
       "      <td>data\\processed\\002_S_1155_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>941_S_4377</td>\n",
       "      <td>F</td>\n",
       "      <td>121.6</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>test</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_4377_f...</td>\n",
       "      <td>data\\processed\\941_S_4377_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>941_S_4420</td>\n",
       "      <td>M</td>\n",
       "      <td>79.4</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>81.5</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_4420_f...</td>\n",
       "      <td>data\\processed\\941_S_4420_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>941_S_4764</td>\n",
       "      <td>F</td>\n",
       "      <td>77.6</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>82.8</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_4764_f...</td>\n",
       "      <td>data\\processed\\941_S_4764_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>941_S_5124</td>\n",
       "      <td>F</td>\n",
       "      <td>78.9</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>76.8</td>\n",
       "      <td>test</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_5124_f...</td>\n",
       "      <td>data\\processed\\941_S_5124_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>941_S_5193</td>\n",
       "      <td>F</td>\n",
       "      <td>88.9</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>72.6</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_5193_f...</td>\n",
       "      <td>data\\processed\\941_S_5193_fused.nii</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Subject Sex  Weight Research Group  APOE A1  APOE A2   Age  \\\n",
       "0    002_S_0295   M    73.0             CN      3.0      4.0  90.0   \n",
       "1    002_S_0413   F    57.6             CN      3.0      3.0  81.5   \n",
       "2    002_S_0685   F    68.9             CN      3.0      3.0  95.8   \n",
       "3    002_S_0729   F    65.8            MCI      3.0      4.0  71.3   \n",
       "4    002_S_1155   M    64.9            MCI      3.0      3.0  64.0   \n",
       "..          ...  ..     ...            ...      ...      ...   ...   \n",
       "173  941_S_4377   F   121.6            MCI      3.0      4.0  69.5   \n",
       "174  941_S_4420   M    79.4            MCI      3.0      3.0  81.5   \n",
       "175  941_S_4764   F    77.6            MCI      3.0      3.0  82.8   \n",
       "176  941_S_5124   F    78.9             CN      3.0      3.0  76.8   \n",
       "177  941_S_5193   F    88.9             CN      3.0      3.0  72.6   \n",
       "\n",
       "    dataset_split                                          File_Path  \\\n",
       "0           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "1            test  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "2            test  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "3      validation  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "4           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "..            ...                                                ...   \n",
       "173          test  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "174         train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "175         train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "176          test  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "177         train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "\n",
       "                                     File_Path_desktop  \\\n",
       "0    D:\\Data\\Preprocessed\\Fused Images\\002_S_0295_f...   \n",
       "1    D:\\Data\\Preprocessed\\Fused Images\\002_S_0413_f...   \n",
       "2    D:\\Data\\Preprocessed\\Fused Images\\002_S_0685_f...   \n",
       "3    D:\\Data\\Preprocessed\\Fused Images\\002_S_0729_f...   \n",
       "4    D:\\Data\\Preprocessed\\Fused Images\\002_S_1155_f...   \n",
       "..                                                 ...   \n",
       "173  D:\\Data\\Preprocessed\\Fused Images\\941_S_4377_f...   \n",
       "174  D:\\Data\\Preprocessed\\Fused Images\\941_S_4420_f...   \n",
       "175  D:\\Data\\Preprocessed\\Fused Images\\941_S_4764_f...   \n",
       "176  D:\\Data\\Preprocessed\\Fused Images\\941_S_5124_f...   \n",
       "177  D:\\Data\\Preprocessed\\Fused Images\\941_S_5193_f...   \n",
       "\n",
       "                                    PATH  \n",
       "0    data\\processed\\002_S_0295_fused.nii  \n",
       "1    data\\processed\\002_S_0413_fused.nii  \n",
       "2    data\\processed\\002_S_0685_fused.nii  \n",
       "3    data\\processed\\002_S_0729_fused.nii  \n",
       "4    data\\processed\\002_S_1155_fused.nii  \n",
       "..                                   ...  \n",
       "173  data\\processed\\941_S_4377_fused.nii  \n",
       "174  data\\processed\\941_S_4420_fused.nii  \n",
       "175  data\\processed\\941_S_4764_fused.nii  \n",
       "176  data\\processed\\941_S_5124_fused.nii  \n",
       "177  data\\processed\\941_S_5193_fused.nii  \n",
       "\n",
       "[178 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "def preprocess_labels(df):\n",
    "    # Mapping similar categories to a single category\n",
    "    label_mapping = {\n",
    "        'EMCI': 'MCI',\n",
    "        'LMCI': 'MCI',\n",
    "        'SMC': 'CN'  # If you want SMC to be considered as CN, include this; remove if not\n",
    "    }\n",
    "    df['Research Group'] = df['Research Group'].replace(label_mapping)\n",
    "    return df\n",
    "\n",
    "preprocess_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df = df[df['Research Group'] != 'AD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to hold the balanced data\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each dataset split\n",
    "for split in df['dataset_split'].unique():\n",
    "    # Filter the DataFrame for the current split\n",
    "    split_df = df[df['dataset_split'] == split]\n",
    "    \n",
    "    # Find the minimum number of rows for any Research Group within this split\n",
    "    min_size = split_df['Research Group'].value_counts().min()\n",
    "    \n",
    "    # Sample from each group to match the minimum size\n",
    "    sampled_groups = [group_df.sample(n=min_size, random_state=1) \n",
    "                      for name, group_df in split_df.groupby('Research Group')]\n",
    "    \n",
    "    # Concatenate the sampled groups into the balanced DataFrame\n",
    "    balanced_df = pd.concat([balanced_df, *sampled_groups], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Research Group\n",
       "CN     64\n",
       "MCI    64\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = balanced_df\n",
    "df['Research Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Research Group</th>\n",
       "      <th>APOE A1</th>\n",
       "      <th>APOE A2</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>File_Path</th>\n",
       "      <th>File_Path_desktop</th>\n",
       "      <th>PATH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002_S_1280</td>\n",
       "      <td>F</td>\n",
       "      <td>89.4</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>75.1</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_1280_f...</td>\n",
       "      <td>data\\processed\\002_S_1280_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128_S_0863</td>\n",
       "      <td>M</td>\n",
       "      <td>92.1</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>79.3</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\128_S_0863_f...</td>\n",
       "      <td>data\\processed\\128_S_0863_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002_S_4213</td>\n",
       "      <td>F</td>\n",
       "      <td>80.0</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>78.1</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_4213_f...</td>\n",
       "      <td>data\\processed\\002_S_4213_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>032_S_5289</td>\n",
       "      <td>F</td>\n",
       "      <td>79.4</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>59.8</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\032_S_5289_f...</td>\n",
       "      <td>data\\processed\\032_S_5289_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>053_S_5296</td>\n",
       "      <td>M</td>\n",
       "      <td>76.0</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>69.3</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\053_S_5296_f...</td>\n",
       "      <td>data\\processed\\053_S_5296_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>002_S_4447</td>\n",
       "      <td>F</td>\n",
       "      <td>68.5</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69.7</td>\n",
       "      <td>validation</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_4447_f...</td>\n",
       "      <td>data\\processed\\002_S_4447_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>022_S_2167</td>\n",
       "      <td>M</td>\n",
       "      <td>71.4</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>83.2</td>\n",
       "      <td>validation</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\022_S_2167_f...</td>\n",
       "      <td>data\\processed\\022_S_2167_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>022_S_4805</td>\n",
       "      <td>F</td>\n",
       "      <td>43.5</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>72.2</td>\n",
       "      <td>validation</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\022_S_4805_f...</td>\n",
       "      <td>data\\processed\\022_S_4805_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>002_S_4251</td>\n",
       "      <td>M</td>\n",
       "      <td>78.0</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>validation</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_4251_f...</td>\n",
       "      <td>data\\processed\\002_S_4251_fused.nii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>128_S_4553</td>\n",
       "      <td>F</td>\n",
       "      <td>54.9</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68.9</td>\n",
       "      <td>validation</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\128_S_4553_f...</td>\n",
       "      <td>data\\processed\\128_S_4553_fused.nii</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Subject Sex  Weight Research Group  APOE A1  APOE A2   Age  \\\n",
       "0    002_S_1280   F    89.4             CN      3.0      4.0  75.1   \n",
       "1    128_S_0863   M    92.1             CN      3.0      3.0  79.3   \n",
       "2    002_S_4213   F    80.0             CN      3.0      3.0  78.1   \n",
       "3    032_S_5289   F    79.4             CN      3.0      4.0  59.8   \n",
       "4    053_S_5296   M    76.0             CN      3.0      3.0  69.3   \n",
       "..          ...  ..     ...            ...      ...      ...   ...   \n",
       "123  002_S_4447   F    68.5            MCI      3.0      4.0  69.7   \n",
       "124  022_S_2167   M    71.4            MCI      3.0      3.0  83.2   \n",
       "125  022_S_4805   F    43.5            MCI      3.0      4.0  72.2   \n",
       "126  002_S_4251   M    78.0            MCI      3.0      3.0  72.0   \n",
       "127  128_S_4553   F    54.9            MCI      3.0      3.0  68.9   \n",
       "\n",
       "    dataset_split                                          File_Path  \\\n",
       "0           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "1           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "2           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "3           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "4           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "..            ...                                                ...   \n",
       "123    validation  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "124    validation  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "125    validation  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "126    validation  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "127    validation  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "\n",
       "                                     File_Path_desktop  \\\n",
       "0    D:\\Data\\Preprocessed\\Fused Images\\002_S_1280_f...   \n",
       "1    D:\\Data\\Preprocessed\\Fused Images\\128_S_0863_f...   \n",
       "2    D:\\Data\\Preprocessed\\Fused Images\\002_S_4213_f...   \n",
       "3    D:\\Data\\Preprocessed\\Fused Images\\032_S_5289_f...   \n",
       "4    D:\\Data\\Preprocessed\\Fused Images\\053_S_5296_f...   \n",
       "..                                                 ...   \n",
       "123  D:\\Data\\Preprocessed\\Fused Images\\002_S_4447_f...   \n",
       "124  D:\\Data\\Preprocessed\\Fused Images\\022_S_2167_f...   \n",
       "125  D:\\Data\\Preprocessed\\Fused Images\\022_S_4805_f...   \n",
       "126  D:\\Data\\Preprocessed\\Fused Images\\002_S_4251_f...   \n",
       "127  D:\\Data\\Preprocessed\\Fused Images\\128_S_4553_f...   \n",
       "\n",
       "                                    PATH  \n",
       "0    data\\processed\\002_S_1280_fused.nii  \n",
       "1    data\\processed\\128_S_0863_fused.nii  \n",
       "2    data\\processed\\002_S_4213_fused.nii  \n",
       "3    data\\processed\\032_S_5289_fused.nii  \n",
       "4    data\\processed\\053_S_5296_fused.nii  \n",
       "..                                   ...  \n",
       "123  data\\processed\\002_S_4447_fused.nii  \n",
       "124  data\\processed\\022_S_2167_fused.nii  \n",
       "125  data\\processed\\022_S_4805_fused.nii  \n",
       "126  data\\processed\\002_S_4251_fused.nii  \n",
       "127  data\\processed\\128_S_4553_fused.nii  \n",
       "\n",
       "[128 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame loaded with the 'Research Group' column available\n",
    "label_categories = pd.Categorical(df['Research Group'])\n",
    "label_mapping = {code: category for code, category in enumerate(label_categories.categories)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiiDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object.\n",
    "        :param df: DataFrame containing file paths, labels, and subject IDs.\n",
    "        :param transform: A function or a series of transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        self.paths = df['PATH'].tolist()  # Paths to .nii files\n",
    "        # Convert labels to categorical codes and maintain a mapping\n",
    "        self.label_mapping = {category: code for code, category in enumerate(pd.Categorical(df['Research Group']).categories)}\n",
    "        self.labels = pd.Categorical(df['Research Group'], categories=self.label_mapping.keys()).codes\n",
    "        self.subjects = df['Subject'].tolist()  # Subject identifiers\n",
    "        self.transform = transform  # Transformation function(s)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        try:\n",
    "            image = nib.load(path).get_fdata()\n",
    "            image = np.expand_dims(image, axis=0)  # Add a channel dimension\n",
    "            image = image.astype(np.float32)  # Ensure the image is float32\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading NII file at {path}: {e}\")\n",
    "            image = np.zeros((1, 64, 64, 64), dtype=np.float32)  # Fallback image of the same type\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        subject = self.subjects[idx]\n",
    "\n",
    "        return image, label, path, subject\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def load_datasets(df):\n",
    "    train_df = df[df['dataset_split'] == 'train']\n",
    "    val_df = df[df['dataset_split'] == 'validation']\n",
    "    test_df = df[df['dataset_split'] == 'test']\n",
    "    \n",
    "    train_dataset = NiiDataset(train_df)\n",
    "    val_dataset = NiiDataset(val_df)\n",
    "    test_dataset = NiiDataset(test_df)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=4):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class Baseline3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2, init_filters=32, kernel_size=3, stride=2, num_fc_units=128):\n",
    "        super(Baseline3DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, init_filters, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv3d(init_filters, init_filters*2, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv3 = nn.Conv3d(init_filters*2, init_filters*4, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Compute the flattened size after all convolutions and pooling\n",
    "        self.final_dim = self._get_conv_output_dim(193, 3, stride, kernel_size, init_filters*4)\n",
    "        self.fc1 = nn.Linear(self.final_dim, num_fc_units)\n",
    "        self.fc2 = nn.Linear(num_fc_units, num_classes)\n",
    "\n",
    "    def _get_conv_output_dim(self, input_dim, num_convs, stride, kernel_size, num_filters):\n",
    "        output_dim = input_dim\n",
    "        for _ in range(num_convs):\n",
    "            output_dim = ((output_dim - kernel_size + 2 * (kernel_size // 2)) // stride + 1) // 2  # Pooling divides size by 2\n",
    "        return output_dim * output_dim * output_dim * num_filters\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, label_mapping, num_epochs=10, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_results = []\n",
    "    train_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        epoch_losses = []\n",
    "        for images, labels, paths, subjects in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted_indices == labels).sum().item()\n",
    "            predicted_labels = [label_mapping[code] for code in predicted_indices.cpu().numpy()]\n",
    "\n",
    "            for label, pred, path, subject in zip(labels.cpu().numpy(), predicted_labels, paths, subjects):\n",
    "                train_results.append({\n",
    "                    'Subject': subject,\n",
    "                    'Path': path,\n",
    "                    'Actual Label': label_mapping[label.item()],\n",
    "                    'Prediction': pred,\n",
    "                    'Type': 'Train'\n",
    "                })\n",
    "        \n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        print(f\"Average loss for Epoch {epoch+1}: {avg_loss:.4f} - Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    return train_results, train_accuracies\n",
    "\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, criterion, label_mapping, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    validation_results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths, subjects in tqdm(val_loader, desc='Validation'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted_indices == labels).sum().item()\n",
    "            predicted_labels = [label_mapping[code] for code in predicted_indices.cpu().numpy()]\n",
    "\n",
    "            for label, pred, path, subject in zip(labels.cpu().numpy(), predicted_labels, paths, subjects):\n",
    "                validation_results.append({\n",
    "                    'Subject': subject,\n",
    "                    'Path': path,\n",
    "                    'Actual Label': label_mapping[label.item()],\n",
    "                    'Prediction': pred,\n",
    "                    'Type': 'Validation'\n",
    "                })\n",
    "    accuracy = 100 * correct / total\n",
    "    return validation_results, accuracy\n",
    "\n",
    "def test_model(model, test_loader, label_mapping, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths, subjects in tqdm(test_loader, desc='Testing'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted_indices == labels).sum().item()\n",
    "            predicted_labels = [label_mapping[code] for code in predicted_indices.cpu().numpy()]\n",
    "\n",
    "            for label, pred, path, subject in zip(labels.cpu().numpy(), predicted_labels, paths, subjects):\n",
    "                test_results.append({\n",
    "                    'Subject': subject,\n",
    "                    'Path': path,\n",
    "                    'Actual Label': label_mapping[label.item()],\n",
    "                    'Prediction': pred,\n",
    "                    'Type': 'Test'\n",
    "                })\n",
    "    accuracy = 100 * correct / total\n",
    "    return test_results, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  43%|████▎     | 33/76 [00:02<00:03, 11.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 66\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Example of how to call run_experiment\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Assuming 'df' has been preprocessed already\u001b[39;00m\n\u001b[0;32m     55\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_filters\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     65\u001b[0m }\n\u001b[1;32m---> 66\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 18\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(df, config)\u001b[0m\n\u001b[0;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Training and validation\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m train_results, train_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m validate_results, val_accuracy \u001b[38;5;241m=\u001b[39m validate_model(model, val_loader, criterion, label_mapping, device)\n\u001b[0;32m     20\u001b[0m test_results, test_accuracy \u001b[38;5;241m=\u001b[39m test_model(model, test_loader, label_mapping, device)\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, label_mapping, num_epochs, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels, paths, subjects \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     11\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m, in \u001b[0;36mNiiDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     24\u001b[0m image \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(path)\u001b[38;5;241m.\u001b[39mget_fdata()\n\u001b[0;32m     25\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add a channel dimension\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure the image is float32\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     28\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def run_experiment(df, config):\n",
    "    \"\"\"Run the experiment with the given configuration on the preprocessed DataFrame.\"\"\"\n",
    "    train_dataset, val_dataset, test_dataset = load_datasets(df)\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Baseline3DCNN(num_classes=config['num_classes'], init_filters=config['init_filters'],\n",
    "                          kernel_size=config['kernel_size'], stride=config['stride'], num_fc_units=config['num_fc_units']).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training and validation\n",
    "    train_results, train_accuracies = train_model(model, train_loader, criterion, optimizer, label_mapping, config['num_epochs'], device)\n",
    "    validate_results, val_accuracy = validate_model(model, val_loader, criterion, label_mapping, device)\n",
    "    test_results, test_accuracy = test_model(model, test_loader, label_mapping, device)\n",
    "    \n",
    "    # Save detailed results to Excel\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = os.path.join('reports', f'{formatted_time}_Experiment.xlsx')\n",
    "    \n",
    "    summary_data = {\n",
    "        'Phase': ['Training', 'Validation', 'Testing'],\n",
    "        'Accuracy': [train_accuracies[-1], val_accuracy, test_accuracy]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    all_results = pd.DataFrame(train_results + validate_results + test_results)\n",
    "    config_df = pd.DataFrame([config])\n",
    "    \n",
    "    with pd.ExcelWriter(filename) as writer:\n",
    "        config_df.to_excel(writer, sheet_name='Configuration')\n",
    "        all_results.to_excel(writer, sheet_name='Results')\n",
    "        summary_df.to_excel(writer, sheet_name='Summary')\n",
    "\n",
    "    # Append a summary of this experiment to the cumulative RESULTS.xlsx file\n",
    "    results_file = os.path.join('reports', 'RESULTS.xlsx')\n",
    "    experiment_summary = {**config, **{'Training Accuracy': train_accuracies[-1], 'Validation Accuracy': val_accuracy, 'Test Accuracy': test_accuracy, 'DATETIME': formatted_time}}\n",
    "    summary_row = pd.DataFrame([experiment_summary])\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        with pd.ExcelWriter(results_file, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "            summary_row.to_excel(writer, startrow=writer.sheets['Sheet1'].max_row, index=False, header=False)\n",
    "    else:\n",
    "        summary_row.to_excel(results_file, index=False)\n",
    "\n",
    "    return filename, train_accuracies[-1], val_accuracy, test_accuracy\n",
    "\n",
    "# Example of how to call run_experiment\n",
    "# Assuming 'df' has been preprocessed already\n",
    "config = {\n",
    "    'num_classes': 2,\n",
    "    'init_filters': 32,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'num_fc_units': 128,\n",
    "    'optimizer': 'Adam',\n",
    "    'loss_criterion': 'BCEWithLogitsLoss',\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 1\n",
    "}\n",
    "run_experiment(df, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary3DCNN(nn.Module):\n",
    "    def __init__(self, init_filters=32, kernel_size=3, stride=2, num_fc_units=128):\n",
    "        super(Binary3DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, init_filters, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(init_filters)\n",
    "        self.conv2 = nn.Conv3d(init_filters, init_filters*2, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(init_filters*2)\n",
    "        self.conv3 = nn.Conv3d(init_filters*2, init_filters*4, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(init_filters*4)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        \n",
    "        # Calculate the final dimensions after all convolutions and pooling\n",
    "        self.final_dim = self._get_conv_output_dim(193, 3, stride, kernel_size, init_filters * 4)  # 193 is an example, adjust based on your input dimension\n",
    "        self.fc1 = nn.Linear(self.final_dim, num_fc_units)\n",
    "        self.fc2 = nn.Linear(num_fc_units, 1)  # Only 1 output unit for binary classification\n",
    "\n",
    "    def _get_conv_output_dim(self, input_dim, num_convs, stride, kernel_size, num_filters):\n",
    "        output_dim = input_dim\n",
    "        for _ in range(num_convs):\n",
    "            output_dim = ((output_dim - kernel_size + 2 * (kernel_size // 2)) // stride + 1) // 2  # Pooling divides size by 2\n",
    "        return output_dim * output_dim * output_dim * num_filters\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.leaky_relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.leaky_relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.leaky_relu(self.bn3(self.conv3(x))))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_results = []\n",
    "    train_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        epoch_losses = []\n",
    "        for images, labels, paths, subjects in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.view_as(outputs))  # Ensure label shape matches output shape\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            # Update correct/total for accuracy computation\n",
    "            predicted = (outputs > 0).long()  # Binary prediction based on threshold at 0\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        print(f\"Average loss for Epoch {epoch+1}: {avg_loss:.4f} - Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    return train_results, train_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiiDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object.\n",
    "        :param df: DataFrame containing file paths, labels, and subject IDs.\n",
    "        :param transform: A function or a series of transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        self.paths = df['PATH'].tolist()  # Paths to .nii files\n",
    "        # Convert labels to binary format and ensure they are floats\n",
    "        self.labels = pd.get_dummies(df['Research Group'], drop_first=True).values.astype(float)\n",
    "        self.subjects = df['Subject'].tolist()  # Subject identifiers\n",
    "        self.transform = transform  # Transformation function(s)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        try:\n",
    "            image = nib.load(path).get_fdata()\n",
    "            image = np.expand_dims(image, axis=0)  # Add a channel dimension\n",
    "            image = image.astype(np.float32)  # Ensure the image is float32\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading NII file at {path}: {e}\")\n",
    "            image = np.zeros((1, 64, 64, 64), dtype=np.float32)  # Fallback image of the same type\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        subject = self.subjects[idx]\n",
    "\n",
    "        return image, label, path, subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/76 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/76 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 79\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Example of how to call run_experiment\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Assuming 'df' has been preprocessed already\u001b[39;00m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_filters\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     78\u001b[0m }\n\u001b[1;32m---> 79\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 32\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(df, config)\u001b[0m\n\u001b[0;32m     29\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Training and validation\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m train_results, train_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m validate_results, val_accuracy \u001b[38;5;241m=\u001b[39m validate_model(model, val_loader, criterion, device)\n\u001b[0;32m     34\u001b[0m test_results, test_accuracy \u001b[38;5;241m=\u001b[39m test_model(model, test_loader, device)\n",
      "Cell \u001b[1;32mIn[26], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mview_as(outputs))  \u001b[38;5;66;03m# Ensure label shape matches output shape\u001b[39;00m\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 24\u001b[0m, in \u001b[0;36mBinary3DCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))))\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))))\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def run_experiment(df, config):\n",
    "    \"\"\"Run the experiment with the given configuration on the preprocessed DataFrame.\"\"\"\n",
    "    train_dataset, val_dataset, test_dataset = load_datasets(df)\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = Binary3DCNN(init_filters=config['init_filters'],\n",
    "                        kernel_size=config['kernel_size'], stride=config['stride'], num_fc_units=config['num_fc_units']).to(device)\n",
    "    \n",
    "    if config['loss_criterion'] == 'BCEWithLogitsLoss':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss function specified\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training and validation\n",
    "    train_results, train_accuracies = train_model(model, train_loader, criterion, optimizer, config['num_epochs'], device)\n",
    "    validate_results, val_accuracy = validate_model(model, val_loader, criterion, device)\n",
    "    test_results, test_accuracy = test_model(model, test_loader, device)\n",
    "    \n",
    "    # Save detailed results to Excel\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = os.path.join('reports', f'{formatted_time}_Experiment.xlsx')\n",
    "    \n",
    "    summary_data = {\n",
    "        'Phase': ['Training', 'Validation', 'Testing'],\n",
    "        'Accuracy': [train_accuracies[-1], val_accuracy, test_accuracy]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    all_results = pd.DataFrame(train_results + validate_results + test_results)\n",
    "    config_df = pd.DataFrame([config])\n",
    "    \n",
    "    with pd.ExcelWriter(filename) as writer:\n",
    "        config_df.to_excel(writer, sheet_name='Configuration')\n",
    "        all_results.to_excel(writer, sheet_name='Results')\n",
    "        summary_df.to_excel(writer, sheet_name='Summary')\n",
    "\n",
    "    # Append a summary of this experiment to the cumulative RESULTS.xlsx file\n",
    "    results_file = os.path.join('reports', 'RESULTS.xlsx')\n",
    "    experiment_summary = {**config, **{'Training Accuracy': train_accuracies[-1], 'Validation Accuracy': val_accuracy, 'Test Accuracy': test_accuracy, 'DATETIME': formatted_time}}\n",
    "    summary_row = pd.DataFrame([experiment_summary])\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        with pd.ExcelWriter(results_file, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "            summary_row.to_excel(writer, startrow=writer.sheets['Sheet1'].max_row, index=False, header=False)\n",
    "    else:\n",
    "        summary_row.to_excel(results_file, index=False)\n",
    "\n",
    "    return filename, train_accuracies[-1], val_accuracy, test_accuracy\n",
    "\n",
    "# Example of how to call run_experiment\n",
    "# Assuming 'df' has been preprocessed already\n",
    "config = {\n",
    "    'init_filters': 32,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'num_fc_units': 128,\n",
    "    'optimizer': 'Adam',\n",
    "    'loss_criterion': 'BCEWithLogitsLoss',\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 1\n",
    "}\n",
    "run_experiment(df, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
