{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Specify the full path to the module file\n",
    "module_path = 'D:\\\\Github Folder\\\\MasterThesis\\\\notebooks\\\\FUNCTIONS.py'\n",
    "\n",
    "# Load the module\n",
    "spec = importlib.util.spec_from_file_location(\"FUNCTIONS\", module_path)\n",
    "functions = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(functions)\n",
    "\n",
    "# Now you can use the functions as if you had imported them\n",
    "load_datasets = functions.load_datasets\n",
    "create_dataloaders = functions.create_dataloaders\n",
    "train_and_validate = functions.train_and_validate\n",
    "test_model = functions.test_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"references\\Subject_info_balanced.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame loaded with the 'Research Group' column available\n",
    "label_categories = pd.Categorical(df['Research Group'])\n",
    "label_mapping = {code: category for code, category in enumerate(label_categories.categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, label_mapping, num_epochs=10, patience=5, device='cuda'):\n",
    "    model.to(device)\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    val_losses = []  # To store validation losses for monitoring\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0  # Initialize the best validation accuracy\n",
    "    epochs_no_improve_loss = 0  # Counter for epochs with no improvement in loss\n",
    "    epochs_no_improve_acc = 0  # Counter for epochs with no improvement in accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        train_epoch_losses = []\n",
    "        \n",
    "        for images, labels, _, _ in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Train'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.unsqueeze(1)  # Change shape from [batch_size] to [batch_size, 1]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.float())  # Ensure labels are float for BCEWithLogitsLoss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_epoch_losses.append(loss.item())\n",
    "\n",
    "            predicted_indices = outputs > 0  # Using 0 as threshold to determine class\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted_indices == labels).sum().item()\n",
    "\n",
    "        train_avg_loss = sum(train_epoch_losses) / len(train_epoch_losses)\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_avg_loss:.4f} - Train Accuracy: {train_accuracy:.2f}%')\n",
    "        \n",
    "        # Validation phase at the end of each epoch\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_epoch_losses = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels, _, _ in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validate'):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                labels = labels.unsqueeze(1)  # Change shape from [batch_size] to [batch_size, 1]\n",
    "                outputs = model(images)\n",
    "                val_loss = criterion(outputs, labels.float())\n",
    "                val_epoch_losses.append(val_loss.item())\n",
    "                predicted_indices = outputs > 0  # Using 0 as threshold to determine class\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted_indices == labels).sum().item()\n",
    "\n",
    "        val_avg_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n",
    "        val_losses.append(val_avg_loss)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f'Epoch {epoch+1}: Validation Loss: {val_avg_loss:.4f} - Validation Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "        # Early stopping logic based on loss\n",
    "        if val_avg_loss < best_val_loss:\n",
    "            best_val_loss = val_avg_loss\n",
    "            epochs_no_improve_loss = 0\n",
    "        else:\n",
    "            epochs_no_improve_loss += 1\n",
    "        \n",
    "        # Early stopping logic based on accuracy\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_no_improve_acc = 0\n",
    "        else:\n",
    "            epochs_no_improve_acc += 1\n",
    "        \n",
    "        # Check for early stop condition\n",
    "        if epochs_no_improve_loss >= patience or epochs_no_improve_acc >= patience:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs due to no improvement in validation loss or accuracy.')\n",
    "            break  # Break out of the loop if no improvement for 'patience' consecutive epochs\n",
    "\n",
    "    return train_accuracies, val_accuracies, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG3D, self).__init__()\n",
    "        # Define 3D convolution blocks with BatchNorm and ReLU activation\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2, padding=(1, 0, 1))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2, padding=(1, 1, 0))\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2, padding=(1, 0, 1))\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2, padding=(1, 1, 0))\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv3d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool5 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Pre-compute the flat features dimension after all pooling layers\n",
    "        self.flat_features = self._calculate_flat_features(1, 193, 229, 193)\n",
    "\n",
    "        # Define fully connected layers with BatchNorm, ReLU, and Dropout\n",
    "        self.fc6 = nn.Sequential(\n",
    "            nn.Linear(self.flat_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.fc7 = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        self.fc8 = nn.Linear(512, 1)\n",
    "\n",
    "    def _calculate_flat_features(self, channels, depth, height, width):\n",
    "        x = torch.zeros((1, channels, depth, height, width))\n",
    "        x = self.pool1(self.conv1(x))\n",
    "        x = self.pool2(self.conv2(x))\n",
    "        x = self.pool3(self.conv3(x))\n",
    "        x = self.pool4(self.conv4(x))\n",
    "        x = self.pool5(self.conv5(x))\n",
    "        return int(x.numel() // x.size(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.conv1(x))\n",
    "        x = self.pool2(self.conv2(x))\n",
    "        x = self.pool3(self.conv3(x))\n",
    "        x = self.pool4(self.conv4(x))\n",
    "        x = self.pool5(self.conv5(x))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc6(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.fc8(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def run_experiment(df, config):\n",
    "    \"\"\"Run the experiment with the given configuration on the preprocessed DataFrame.\"\"\"\n",
    "    train_dataset, val_dataset, test_dataset = load_datasets(df, config['image_type'])\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = VGG3D().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "\n",
    "    # Training and validation\n",
    "    train_accuracies, val_accuracies, val_losses = train_and_validate(model, train_loader, val_loader, criterion, optimizer, label_mapping, config['num_epochs'], config['patience'], device)\n",
    "    test_results, test_accuracy = test_model(model, test_loader, label_mapping, device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "  # Save detailed results to Excel\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = os.path.join('reports', f'{formatted_time}_Experiment.xlsx')\n",
    "    \n",
    "    summary_data = {\n",
    "        'Phase': ['Training', 'Validation', 'Testing'],\n",
    "        'Accuracy': [train_accuracies[-1], val_accuracies[-1], test_accuracy]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    all_results = pd.DataFrame(test_results)\n",
    "    config_df = pd.DataFrame([config])\n",
    "    \n",
    "    with pd.ExcelWriter(filename) as writer:\n",
    "        config_df.to_excel(writer, sheet_name='Configuration')\n",
    "        all_results.to_excel(writer, sheet_name='Results')\n",
    "        summary_df.to_excel(writer, sheet_name='Summary')\n",
    "\n",
    "    # Append a summary of this experiment to the cumulative RESULTS.xlsx file\n",
    "    results_file = os.path.join('reports', 'RESULTS.xlsx')\n",
    "    experiment_summary = {**config, **{'Training Accuracy': train_accuracies[-1], 'Validation Accuracy': val_accuracies[-1], 'Test Accuracy': test_accuracy, 'DATETIME': formatted_time}}\n",
    "    summary_row = pd.DataFrame([experiment_summary])\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        with pd.ExcelWriter(results_file, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "            existing_df = pd.read_excel(results_file)\n",
    "            # Reindex the existing DataFrame to ensure all columns are aligned and add new columns if necessary\n",
    "            combined_df = pd.concat([existing_df, summary_row], ignore_index=True)\n",
    "            combined_df = combined_df.reindex(columns=(existing_df.columns.tolist() + [col for col in summary_row.columns if col not in existing_df.columns]))\n",
    "            combined_df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "    else:\n",
    "        summary_row.to_excel(results_file, index=False)\n",
    "\n",
    "    return filename, train_accuracies[-1], val_accuracies[-1], test_accuracy\n",
    "\n",
    "# Example configuration and use case\n",
    "config = {\n",
    "    'optimizer': 'Adam',\n",
    "    'loss_criterion': 'BCEWithLogitsLoss',\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 2,\n",
    "    'patience': 4,\n",
    "    'Description' : 'VG33D advanced',\n",
    "    'lr' : 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on image type: MRI_PET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "image_types = ['MRI_PET', 'MRI', 'PET']\n",
    "results = []\n",
    "for image_type in image_types:\n",
    "    config['image_type'] = image_type\n",
    "    print(f\"Working on image type: {image_type}\")\n",
    "    result = run_experiment(df, config)\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
