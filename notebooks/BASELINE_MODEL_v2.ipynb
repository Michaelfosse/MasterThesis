{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania\\MsC Business Analytics\\Master Thesis\\Python\\Subject_info.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Research Group</th>\n",
       "      <th>APOE A1</th>\n",
       "      <th>APOE A2</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>File_Path</th>\n",
       "      <th>File_Path_desktop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002_S_0295</td>\n",
       "      <td>M</td>\n",
       "      <td>73.0</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_0295_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_S_0413</td>\n",
       "      <td>F</td>\n",
       "      <td>57.6</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>81.5</td>\n",
       "      <td>test</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_0413_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002_S_0685</td>\n",
       "      <td>F</td>\n",
       "      <td>68.9</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>95.8</td>\n",
       "      <td>test</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_0685_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002_S_0729</td>\n",
       "      <td>F</td>\n",
       "      <td>65.8</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>71.3</td>\n",
       "      <td>validation</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_0729_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002_S_1155</td>\n",
       "      <td>M</td>\n",
       "      <td>64.9</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\002_S_1155_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>941_S_4377</td>\n",
       "      <td>F</td>\n",
       "      <td>121.6</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>test</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_4377_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>941_S_4420</td>\n",
       "      <td>M</td>\n",
       "      <td>79.4</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>81.5</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_4420_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>941_S_4764</td>\n",
       "      <td>F</td>\n",
       "      <td>77.6</td>\n",
       "      <td>MCI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>82.8</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_4764_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>941_S_5124</td>\n",
       "      <td>F</td>\n",
       "      <td>78.9</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>76.8</td>\n",
       "      <td>test</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_5124_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>941_S_5193</td>\n",
       "      <td>F</td>\n",
       "      <td>88.9</td>\n",
       "      <td>CN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>72.6</td>\n",
       "      <td>train</td>\n",
       "      <td>C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...</td>\n",
       "      <td>D:\\Data\\Preprocessed\\Fused Images\\941_S_5193_f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Subject Sex  Weight Research Group  APOE A1  APOE A2   Age  \\\n",
       "0    002_S_0295   M    73.0             CN      3.0      4.0  90.0   \n",
       "1    002_S_0413   F    57.6             CN      3.0      3.0  81.5   \n",
       "2    002_S_0685   F    68.9             CN      3.0      3.0  95.8   \n",
       "3    002_S_0729   F    65.8            MCI      3.0      4.0  71.3   \n",
       "4    002_S_1155   M    64.9            MCI      3.0      3.0  64.0   \n",
       "..          ...  ..     ...            ...      ...      ...   ...   \n",
       "173  941_S_4377   F   121.6            MCI      3.0      4.0  69.5   \n",
       "174  941_S_4420   M    79.4            MCI      3.0      3.0  81.5   \n",
       "175  941_S_4764   F    77.6            MCI      3.0      3.0  82.8   \n",
       "176  941_S_5124   F    78.9             CN      3.0      3.0  76.8   \n",
       "177  941_S_5193   F    88.9             CN      3.0      3.0  72.6   \n",
       "\n",
       "    dataset_split                                          File_Path  \\\n",
       "0           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "1            test  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "2            test  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "3      validation  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "4           train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "..            ...                                                ...   \n",
       "173          test  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "174         train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "175         train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "176          test  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "177         train  C:\\Users\\Micha\\OneDrive - Høyskolen Kristiania...   \n",
       "\n",
       "                                     File_Path_desktop  \n",
       "0    D:\\Data\\Preprocessed\\Fused Images\\002_S_0295_f...  \n",
       "1    D:\\Data\\Preprocessed\\Fused Images\\002_S_0413_f...  \n",
       "2    D:\\Data\\Preprocessed\\Fused Images\\002_S_0685_f...  \n",
       "3    D:\\Data\\Preprocessed\\Fused Images\\002_S_0729_f...  \n",
       "4    D:\\Data\\Preprocessed\\Fused Images\\002_S_1155_f...  \n",
       "..                                                 ...  \n",
       "173  D:\\Data\\Preprocessed\\Fused Images\\941_S_4377_f...  \n",
       "174  D:\\Data\\Preprocessed\\Fused Images\\941_S_4420_f...  \n",
       "175  D:\\Data\\Preprocessed\\Fused Images\\941_S_4764_f...  \n",
       "176  D:\\Data\\Preprocessed\\Fused Images\\941_S_5124_f...  \n",
       "177  D:\\Data\\Preprocessed\\Fused Images\\941_S_5193_f...  \n",
       "\n",
       "[178 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "def preprocess_labels(df):\n",
    "    # Mapping similar categories to a single category\n",
    "    label_mapping = {\n",
    "        'EMCI': 'MCI',\n",
    "        'LMCI': 'MCI',\n",
    "        'SMC': 'CN'  # If you want SMC to be considered as CN, include this; remove if not\n",
    "    }\n",
    "    df['Research Group'] = df['Research Group'].replace(label_mapping)\n",
    "    return df\n",
    "\n",
    "preprocess_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df = df[df['Research Group'] != 'AD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiiDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object.\n",
    "        :param df: DataFrame containing file paths, labels, and subject IDs.\n",
    "        :param transform: A function or a series of transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        self.paths = df['File_Path_desktop'].tolist()  # Paths to .nii files\n",
    "        # Convert labels to categorical codes and maintain a mapping\n",
    "        self.label_mapping = {category: code for code, category in enumerate(pd.Categorical(df['Research Group']).categories)}\n",
    "        self.labels = pd.Categorical(df['Research Group'], categories=self.label_mapping.keys()).codes\n",
    "        self.subjects = df['Subject'].tolist()  # Subject identifiers\n",
    "        self.transform = transform  # Transformation function(s)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the nii image at the specified index, applies transformations, and returns it along with its label and subject ID.\n",
    "        :param idx: Index of the image to retrieve.\n",
    "        :return: tuple containing the transformed image, its numeric label, the file path, and the subject ID.\n",
    "        \"\"\"\n",
    "        path = self.paths[idx]\n",
    "        image = nib.load(path).get_fdata()  # Load the image data\n",
    "        image = np.expand_dims(image, axis=0)  # Add a channel dimension\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformation\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        subject = self.subjects[idx]  # Retrieve the subject ID\n",
    "        \n",
    "        return image, label, subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def load_datasets(df):\n",
    "    train_df = df[df['dataset_split'] == 'train']\n",
    "    val_df = df[df['dataset_split'] == 'validation']\n",
    "    test_df = df[df['dataset_split'] == 'test']\n",
    "    \n",
    "    train_dataset = NiiDataset(train_df)\n",
    "    val_dataset = NiiDataset(val_df)\n",
    "    test_dataset = NiiDataset(test_df)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=4):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class Baseline3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3, init_filters=32, kernel_size=3, stride=2, num_fc_units=128):\n",
    "        super(Baseline3DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, init_filters, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv3d(init_filters, init_filters*2, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv3 = nn.Conv3d(init_filters*2, init_filters*4, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Compute the flattened size after all convolutions and pooling\n",
    "        self.final_dim = self._get_conv_output_dim(193, 3, stride, kernel_size, init_filters*4)\n",
    "        self.fc1 = nn.Linear(self.final_dim, num_fc_units)\n",
    "        self.fc2 = nn.Linear(num_fc_units, num_classes)\n",
    "\n",
    "    def _get_conv_output_dim(self, input_dim, num_convs, stride, kernel_size, num_filters):\n",
    "        output_dim = input_dim\n",
    "        for _ in range(num_convs):\n",
    "            output_dim = ((output_dim - kernel_size + 2 * (kernel_size // 2)) // stride + 1) // 2  # Pooling divides size by 2\n",
    "        return output_dim * output_dim * output_dim * num_filters\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming Baseline3DCNN is defined as provided above\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model = Baseline3DCNN(num_classes=3, init_filters=32, kernel_size=3, stride=2, num_fc_units=128)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels, subjects in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total = correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, subjects in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = load_datasets(df)  # Create datasets\n",
    "train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=4)  # Create dataloaders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
