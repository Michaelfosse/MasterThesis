{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"references\\Subject_info_balanced.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "class NiiDataset(Dataset):\n",
    "    def __init__(self, df, image_type='MRI_PET', transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object.\n",
    "        :param df: DataFrame containing file paths, labels, and subject IDs.\n",
    "        :param image_type: Type of images to load ('MRI_PET', 'MRI', or 'PET').\n",
    "        :param transform: A function or a series of transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        self.image_type = image_type\n",
    "        if image_type == 'MRI_PET':\n",
    "            self.paths = df['PATH_MRI_PET'].tolist()\n",
    "        elif image_type == 'MRI':\n",
    "            self.paths = df['PATH_MRI'].tolist()\n",
    "        elif image_type == 'PET':\n",
    "            self.paths = df['PATH_PET'].tolist()\n",
    "        self.labels = pd.Categorical(df['Research Group']).codes\n",
    "        self.subjects = df['Subject'].tolist()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve the nth sample from the dataset.\n",
    "        \"\"\"\n",
    "        path = self.paths[idx]\n",
    "        image = self.load_nii(path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        subject = self.subjects[idx]\n",
    "        return image, label, path, subject\n",
    "\n",
    "    def load_nii(self, path):\n",
    "        \"\"\"\n",
    "        Load a NIfTI file and normalize its intensity.\n",
    "        \"\"\"\n",
    "        image = nib.load(path).get_fdata(dtype=np.float32)\n",
    "        image = self.normalize_intensity(image)\n",
    "        image = np.expand_dims(image, axis=0)  # Add a channel dimension\n",
    "        return image\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_intensity(image):\n",
    "        \"\"\"\n",
    "        Normalize the image data to zero mean and unit variance.\n",
    "        \"\"\"\n",
    "        mean_intensity = np.mean(image)\n",
    "        std_intensity = np.std(image)\n",
    "        normalized_image = (image - mean_intensity) / std_intensity\n",
    "        return normalized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def load_datasets(df, image_type):\n",
    "    train_df = df[df['dataset_split'] == 'train']\n",
    "    val_df = df[df['dataset_split'] == 'validation']\n",
    "    test_df = df[df['dataset_split'] == 'test']\n",
    "    \n",
    "    train_dataset = NiiDataset(train_df)\n",
    "    val_dataset = NiiDataset(val_df)\n",
    "    test_dataset = NiiDataset(test_df)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=4):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class Baseline3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2, init_filters=32, kernel_size=3, stride=2, num_fc_units=128):\n",
    "        super(Baseline3DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, init_filters, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv3d(init_filters, init_filters*2, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv3 = nn.Conv3d(init_filters*2, init_filters*4, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Compute the flattened size after all convolutions and pooling\n",
    "        self.final_dim = self._get_conv_output_dim(193, 3, stride, kernel_size, init_filters*4)\n",
    "        self.fc1 = nn.Linear(self.final_dim, num_fc_units)\n",
    "        self.fc2 = nn.Linear(num_fc_units, num_classes)\n",
    "\n",
    "    def _get_conv_output_dim(self, input_dim, num_convs, stride, kernel_size, num_filters):\n",
    "        output_dim = input_dim\n",
    "        for _ in range(num_convs):\n",
    "            output_dim = ((output_dim - kernel_size + 2 * (kernel_size // 2)) // stride + 1) // 2  # Pooling divides size by 2\n",
    "        return output_dim * output_dim * output_dim * num_filters\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame loaded with the 'Research Group' column available\n",
    "label_categories = pd.Categorical(df['Research Group'])\n",
    "label_mapping = {code: category for code, category in enumerate(label_categories.categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, label_mapping, num_epochs=10, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_results = []\n",
    "    train_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        epoch_losses = []\n",
    "        for images, labels, paths, subjects in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted_indices == labels).sum().item()\n",
    "            predicted_labels = [label_mapping[code] for code in predicted_indices.cpu().numpy()]\n",
    "\n",
    "            for label, pred, path, subject in zip(labels.cpu().numpy(), predicted_labels, paths, subjects):\n",
    "                train_results.append({\n",
    "                    'Subject': subject,\n",
    "                    'Path': path,\n",
    "                    'Actual Label': label_mapping[label.item()],\n",
    "                    'Prediction': pred,\n",
    "                    'Type': 'Train'\n",
    "                })\n",
    "        \n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        print(f\"Average loss for Epoch {epoch+1}: {avg_loss:.4f} - Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    return train_results, train_accuracies\n",
    "\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, criterion, label_mapping, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    validation_results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths, subjects in tqdm(val_loader, desc='Validation'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted_indices == labels).sum().item()\n",
    "            predicted_labels = [label_mapping[code] for code in predicted_indices.cpu().numpy()]\n",
    "\n",
    "            for label, pred, path, subject in zip(labels.cpu().numpy(), predicted_labels, paths, subjects):\n",
    "                validation_results.append({\n",
    "                    'Subject': subject,\n",
    "                    'Path': path,\n",
    "                    'Actual Label': label_mapping[label.item()],\n",
    "                    'Prediction': pred,\n",
    "                    'Type': 'Validation'\n",
    "                })\n",
    "    accuracy = 100 * correct / total\n",
    "    return validation_results, accuracy\n",
    "\n",
    "def test_model(model, test_loader, label_mapping, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths, subjects in tqdm(test_loader, desc='Testing'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted_indices == labels).sum().item()\n",
    "            predicted_labels = [label_mapping[code] for code in predicted_indices.cpu().numpy()]\n",
    "\n",
    "            for label, pred, path, subject in zip(labels.cpu().numpy(), predicted_labels, paths, subjects):\n",
    "                test_results.append({\n",
    "                    'Subject': subject,\n",
    "                    'Path': path,\n",
    "                    'Actual Label': label_mapping[label.item()],\n",
    "                    'Prediction': pred,\n",
    "                    'Type': 'Test'\n",
    "                })\n",
    "    accuracy = 100 * correct / total\n",
    "    return test_results, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def run_experiment(df, config):\n",
    "    \"\"\"Run the experiment with the given configuration on the preprocessed DataFrame.\"\"\"\n",
    "    train_dataset, val_dataset, test_dataset = load_datasets(df, config['image_type'])\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # Add the image type to the configuration output in Excel\n",
    "    config['Image Type'] = config['image_type'] \n",
    "    \n",
    "    # Initialize model and training components\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Baseline3DCNN(num_classes=config['num_classes'], init_filters=config['init_filters'],\n",
    "                          kernel_size=config['kernel_size'], stride=config['stride'], num_fc_units=config['num_fc_units']).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training and validation\n",
    "    train_results, train_accuracies = train_model(model, train_loader, criterion, optimizer, label_mapping, config['num_epochs'], device)\n",
    "    validate_results, val_accuracy = validate_model(model, val_loader, criterion, label_mapping, device)\n",
    "    test_results, test_accuracy = test_model(model, test_loader, label_mapping, device)\n",
    "    \n",
    "    # Save detailed results to Excel\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = os.path.join('reports', f'{formatted_time}_Experiment.xlsx')\n",
    "    \n",
    "    summary_data = {\n",
    "        'Phase': ['Training', 'Validation', 'Testing'],\n",
    "        'Accuracy': [train_accuracies[-1], val_accuracy, test_accuracy]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    all_results = pd.DataFrame(train_results + validate_results + test_results)\n",
    "    config_df = pd.DataFrame([config])\n",
    "    \n",
    "    with pd.ExcelWriter(filename) as writer:\n",
    "        config_df.to_excel(writer, sheet_name='Configuration')\n",
    "        all_results.to_excel(writer, sheet_name='Results')\n",
    "        summary_df.to_excel(writer, sheet_name='Summary')\n",
    "\n",
    "    # Append a summary of this experiment to the cumulative RESULTS.xlsx file\n",
    "    results_file = os.path.join('reports', 'RESULTS.xlsx')\n",
    "    experiment_summary = {**config, **{'Training Accuracy': train_accuracies[-1], 'Validation Accuracy': val_accuracy, 'Test Accuracy': test_accuracy, 'DATETIME': formatted_time}}\n",
    "    summary_row = pd.DataFrame([experiment_summary])\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        with pd.ExcelWriter(results_file, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "            summary_row.to_excel(writer, startrow=writer.sheets['Sheet1'].max_row, index=False, header=False)\n",
    "    else:\n",
    "        summary_row.to_excel(results_file, index=False)\n",
    "\n",
    "    return filename, train_accuracies[-1], val_accuracy, test_accuracy\n",
    "\n",
    "# Example of how to call run_experiment\n",
    "# Assuming 'df' has been preprocessed already\n",
    "config = {\n",
    "    'num_classes': 2,\n",
    "    'init_filters': 128,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'num_fc_units': 128,\n",
    "    'optimizer': 'Adam',\n",
    "    'loss_criterion': 'BCEWithLogitsLoss',\n",
    "    'num_epochs': 2,\n",
    "    'batch_size': 1\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 76/76 [00:10<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for Epoch 1: 1.1821 - Accuracy: 48.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 76/76 [00:09<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for Epoch 2: 0.6942 - Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 26/26 [00:03<00:00,  8.46it/s]\n",
      "Testing: 100%|██████████| 26/26 [00:03<00:00,  7.73it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"There is no item named '[Content_Types].xml' in the archive\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[172], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_type \u001b[38;5;129;01min\u001b[39;00m image_types:\n\u001b[0;32m      4\u001b[0m     config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m image_type\n\u001b[1;32m----> 5\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[1;32mIn[171], line 50\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(df, config)\u001b[0m\n\u001b[0;32m     47\u001b[0m summary_row \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([experiment_summary])\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(results_file):\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopenpyxl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_sheet_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverlay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[0;32m     51\u001b[0m         summary_row\u001b[38;5;241m.\u001b[39mto_excel(writer, startrow\u001b[38;5;241m=\u001b[39mwriter\u001b[38;5;241m.\u001b[39msheets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSheet1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax_row, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:75\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenpyxl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_workbook\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_book \u001b[38;5;241m=\u001b[39m load_workbook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles\u001b[38;5;241m.\u001b[39mhandle, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mengine_kwargs)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles\u001b[38;5;241m.\u001b[39mhandle\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\openpyxl\\reader\\excel.py:346\u001b[0m, in \u001b[0;36mload_workbook\u001b[1;34m(filename, read_only, keep_vba, data_only, keep_links, rich_text)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Open the given filename and return the workbook\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03m:param filename: the path to open or a file-like object\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m \n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    344\u001b[0m reader \u001b[38;5;241m=\u001b[39m ExcelReader(filename, read_only, keep_vba,\n\u001b[0;32m    345\u001b[0m                      data_only, keep_links, rich_text)\n\u001b[1;32m--> 346\u001b[0m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mwb\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\openpyxl\\reader\\excel.py:287\u001b[0m, in \u001b[0;36mExcelReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread manifest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_manifest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread strings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_strings()\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\site-packages\\openpyxl\\reader\\excel.py:134\u001b[0m, in \u001b[0;36mExcelReader.read_manifest\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_manifest\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 134\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marchive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mARC_CONTENT_TYPES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     root \u001b[38;5;241m=\u001b[39m fromstring(src)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpackage \u001b[38;5;241m=\u001b[39m Manifest\u001b[38;5;241m.\u001b[39mfrom_tree(root)\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\zipfile.py:1475\u001b[0m, in \u001b[0;36mZipFile.read\u001b[1;34m(self, name, pwd)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, pwd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return file bytes for name.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1475\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m   1476\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\zipfile.py:1514\u001b[0m, in \u001b[0;36mZipFile.open\u001b[1;34m(self, name, mode, pwd, force_zip64)\u001b[0m\n\u001b[0;32m   1511\u001b[0m     zinfo\u001b[38;5;241m.\u001b[39m_compresslevel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompresslevel\n\u001b[0;32m   1512\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m# Get info object for name\u001b[39;00m\n\u001b[1;32m-> 1514\u001b[0m     zinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_to_write(zinfo, force_zip64\u001b[38;5;241m=\u001b[39mforce_zip64)\n",
      "File \u001b[1;32mc:\\Users\\Micha\\.pyenv\\pyenv-win\\versions\\3.10.10\\lib\\zipfile.py:1441\u001b[0m, in \u001b[0;36mZipFile.getinfo\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1439\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNameToInfo\u001b[38;5;241m.\u001b[39mget(name)\n\u001b[0;32m   1440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1441\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m   1442\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThere is no item named \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m in the archive\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "\u001b[1;31mKeyError\u001b[0m: \"There is no item named '[Content_Types].xml' in the archive\""
     ]
    }
   ],
   "source": [
    "image_types = ['MRI_PET', 'MRI', 'PET']\n",
    "results = []\n",
    "for image_type in image_types:\n",
    "    config['image_type'] = image_type\n",
    "    result = run_experiment(df, config)\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def append_to_excel(results_file, new_data):\n",
    "    \"\"\"Appends a DataFrame to an existing Excel file or creates a new one if it doesn't exist.\"\"\"\n",
    "    if os.path.exists(results_file):\n",
    "        book = load_workbook(results_file)\n",
    "        with pd.ExcelWriter(results_file, engine='openpyxl') as writer:\n",
    "            writer.book = book\n",
    "            writer.sheets = {ws.title: ws for ws in book.worksheets}\n",
    "            existing_data = pd.read_excel(results_file)\n",
    "            combined_data = pd.concat([existing_data, new_data], ignore_index=True, sort=False)\n",
    "            combined_data.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "    else:\n",
    "        new_data.to_excel(results_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "def run_experiment(df, config):\n",
    "    \"\"\"Run the experiment with the given configuration on the preprocessed DataFrame.\"\"\"\n",
    "    train_dataset, val_dataset, test_dataset = load_datasets(df, config['image_type'])\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # Add the image type to the configuration output in Excel\n",
    "    config['Image Type'] = config['image_type'] \n",
    "    \n",
    "    # Initialize model and training components\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Baseline3DCNN(num_classes=config['num_classes'], init_filters=config['init_filters'],\n",
    "                          kernel_size=config['kernel_size'], stride=config['stride'], num_fc_units=config['num_fc_units']).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training and validation\n",
    "    train_results, train_accuracies = train_model(model, train_loader, criterion, optimizer, label_mapping, config['num_epochs'], device)\n",
    "    validate_results, val_accuracy = validate_model(model, val_loader, criterion, label_mapping, device)\n",
    "    test_results, test_accuracy = test_model(model, test_loader, label_mapping, device)\n",
    "    \n",
    "    # Save detailed results to Excel\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = os.path.join('reports', f'{formatted_time}_Experiment.xlsx')\n",
    "    \n",
    "    summary_data = {\n",
    "        'Phase': ['Training', 'Validation', 'Testing'],\n",
    "        'Accuracy': [train_accuracies[-1], val_accuracy, test_accuracy]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    all_results = pd.DataFrame(train_results + validate_results + test_results)\n",
    "    config_df = pd.DataFrame([config])\n",
    "    \n",
    "    with pd.ExcelWriter(filename) as writer:\n",
    "        config_df.to_excel(writer, sheet_name='Configuration')\n",
    "        all_results.to_excel(writer, sheet_name='Results')\n",
    "        summary_df.to_excel(writer, sheet_name='Summary')\n",
    "\n",
    "# Append a summary of this experiment to the cumulative RESULTS.xlsx file\n",
    "    results_file = os.path.join('reports', 'RESULTS.xlsx')\n",
    "    experiment_summary = {**config, **{\n",
    "        'Training Accuracy': train_accuracies[-1],\n",
    "        'Validation Accuracy': val_accuracy,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'DATETIME': formatted_time\n",
    "    }}\n",
    "    new_row = pd.DataFrame([experiment_summary])\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        # Load the existing Excel file and sheet\n",
    "        book = load_workbook(results_file)\n",
    "        with pd.ExcelWriter(results_file, engine='openpyxl') as writer:\n",
    "            writer.book = book\n",
    "            writer.sheets = {ws.title: ws for ws in book.worksheets}\n",
    "\n",
    "            # Read existing data\n",
    "            existing_data = pd.read_excel(results_file)\n",
    "            \n",
    "            # Combine new row with existing DataFrame, aligning on columns\n",
    "            combined_data = pd.concat([existing_data, new_row], ignore_index=True, sort=False)\n",
    "\n",
    "            # Write updated DataFrame to Excel, replacing old data\n",
    "            combined_data.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "            writer.save()\n",
    "    else:\n",
    "        # If the file does not exist, simply write the new DataFrame\n",
    "        new_row.to_excel(results_file, index=False)\n",
    "\n",
    "    return filename, train_accuracies[-1], val_accuracy, test_accuracy\n",
    "\n",
    "# Example of how to call run_experiment\n",
    "# Assuming 'df' has been preprocessed already\n",
    "config = {\n",
    "    'num_classes': 2,\n",
    "    'init_filters': 128,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'num_fc_units': 128,\n",
    "    'optimizer': 'Adam',\n",
    "    'loss_criterion': 'BCEWithLogitsLoss',\n",
    "    'num_epochs': 1,\n",
    "    'batch_size': 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "def save_experiment_results(config, train_results, train_accuracies, validate_results, val_accuracy, test_results, test_accuracy):\n",
    "    \"\"\"Saves detailed results of an experiment to an Excel file and appends a summary to a cumulative results file.\"\"\"\n",
    "    # Setup file paths and names\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    detailed_filename = os.path.join('reports', f'{formatted_time}_Experiment.xlsx')\n",
    "    summary_results_file = os.path.join('reports', 'RESULTS.xlsx')\n",
    "\n",
    "    # Prepare dataframes for detailed results\n",
    "    summary_data = {'Phase': ['Training', 'Validation', 'Testing'], 'Accuracy': [train_accuracies[-1], val_accuracy, test_accuracy]}\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    all_results = pd.DataFrame(train_results + validate_results + test_results)\n",
    "    config_df = pd.DataFrame([config])\n",
    "    \n",
    "    # Write detailed results to Excel\n",
    "    with pd.ExcelWriter(detailed_filename) as writer:\n",
    "        config_df.to_excel(writer, sheet_name='Configuration')\n",
    "        all_results.to_excel(writer, sheet_name='Results')\n",
    "        summary_df.to_excel(writer, sheet_name='Summary')\n",
    "\n",
    "    # Prepare summary row for cumulative results\n",
    "    experiment_summary = {**config, **{\n",
    "        'Training Accuracy': train_accuracies[-1],\n",
    "        'Validation Accuracy': val_accuracy,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'DATETIME': formatted_time\n",
    "    }}\n",
    "    summary_row = pd.DataFrame([experiment_summary])\n",
    "\n",
    "    # Append summary row to the cumulative RESULTS.xlsx file\n",
    "    append_to_excel(summary_results_file, summary_row)\n",
    "\n",
    "    return detailed_filename\n",
    "\n",
    "def append_to_excel(results_file, new_data):\n",
    "    \"\"\"Appends a DataFrame to an existing Excel file or creates a new one if it doesn't exist.\"\"\"\n",
    "    if os.path.exists(results_file):\n",
    "        book = load_workbook(results_file)\n",
    "        with pd.ExcelWriter(results_file, engine='openpyxl') as writer:\n",
    "            writer.book = book\n",
    "            writer.sheets = {ws.title: ws for ws in book.worksheets}\n",
    "            existing_data = pd.read_excel(results_file)\n",
    "            combined_data = pd.concat([existing_data, new_data], ignore_index=True, sort=False)\n",
    "            combined_data.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "    else:\n",
    "        new_data.to_excel(results_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(df, config):\n",
    "    \"\"\"Run the experiment with the given configuration on the preprocessed DataFrame.\"\"\"\n",
    "    train_dataset, val_dataset, test_dataset = load_datasets(df, config['image_type'])\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = Baseline3DCNN(num_classes=config['num_classes'], init_filters=config['init_filters'],\n",
    "                          kernel_size=config['kernel_size'], stride=config['stride'], num_fc_units=config['num_fc_units']).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training and validation\n",
    "    train_results, train_accuracies = train_model(model, train_loader, criterion, optimizer, config['num_epochs'], device=device)\n",
    "    validate_results, val_accuracy = validate_model(model, val_loader, criterion, device=device)\n",
    "    test_results, test_accuracy = test_model(model, test_loader, device=device)\n",
    "    \n",
    "    # Save results to Excel\n",
    "    filename = save_experiment_results(config, train_results, train_accuracies, validate_results, val_accuracy, test_results, test_accuracy)\n",
    "\n",
    "    return filename, train_accuracies[-1], val_accuracy, test_accuracy\n",
    "\n",
    "config = {\n",
    "    'num_classes': 2,\n",
    "    'init_filters': 128,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'num_fc_units': 128,\n",
    "    'optimizer': 'Adam',\n",
    "    'loss_criterion': 'BCEWithLogitsLoss',\n",
    "    'num_epochs': 1,\n",
    "    'batch_size': 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
