{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"references\\Subject_info_balanced.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame loaded with the 'Research Group' column available\n",
    "label_categories = pd.Categorical(df['Research Group'])\n",
    "label_mapping = {code: category for code, category in enumerate(label_categories.categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "class NiiDataset(Dataset):\n",
    "    def __init__(self, df, image_type='MRI_PET', transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object.\n",
    "        :param df: DataFrame containing file paths, labels, and subject IDs.\n",
    "        :param image_type: Type of images to load ('MRI_PET', 'MRI', or 'PET').\n",
    "        :param transform: A function or a series of transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        self.image_type = image_type\n",
    "        if image_type == 'MRI_PET':\n",
    "            self.paths = df['PATH_MRI_PET'].tolist()\n",
    "        elif image_type == 'MRI':\n",
    "            self.paths = df['PATH_MRI'].tolist()\n",
    "        elif image_type == 'PET':\n",
    "            self.paths = df['PATH_PET'].tolist()\n",
    "        self.labels = pd.Categorical(df['Research Group']).codes\n",
    "        self.subjects = df['Subject'].tolist()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve the nth sample from the dataset.\n",
    "        \"\"\"\n",
    "        path = self.paths[idx]\n",
    "        image = self.load_nii(path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        subject = self.subjects[idx]\n",
    "        return image, label, path, subject\n",
    "\n",
    "    def load_nii(self, path):\n",
    "        \"\"\"\n",
    "        Load a NIfTI file and normalize its intensity.\n",
    "        \"\"\"\n",
    "        image = nib.load(path).get_fdata(dtype=np.float32)\n",
    "        image = self.normalize_intensity(image)\n",
    "        image = np.expand_dims(image, axis=0)  # Add a channel dimension\n",
    "        return image\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_intensity(image):\n",
    "        \"\"\"\n",
    "        Normalize the image data to zero mean and unit variance.\n",
    "        \"\"\"\n",
    "        mean_intensity = np.mean(image)\n",
    "        std_intensity = np.std(image)\n",
    "        normalized_image = (image - mean_intensity) / std_intensity\n",
    "        return normalized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def load_datasets(df, image_type):\n",
    "    train_df = df[df['dataset_split'] == 'train']\n",
    "    val_df = df[df['dataset_split'] == 'validation']\n",
    "    test_df = df[df['dataset_split'] == 'test']\n",
    "    \n",
    "    train_dataset = NiiDataset(train_df)\n",
    "    val_dataset = NiiDataset(val_df)\n",
    "    test_dataset = NiiDataset(test_df)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=4):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class Baseline3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2, init_filters=32, kernel_size=3, stride=2, num_fc_units=128):\n",
    "        super(Baseline3DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, init_filters, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv3d(init_filters, init_filters*2, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv3 = nn.Conv3d(init_filters*2, init_filters*4, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Compute the flattened size after all convolutions and pooling\n",
    "        self.final_dim = self._get_conv_output_dim(193, 3, stride, kernel_size, init_filters*4)\n",
    "        self.fc1 = nn.Linear(self.final_dim, num_fc_units)\n",
    "        self.fc2 = nn.Linear(num_fc_units, num_classes)\n",
    "\n",
    "    def _get_conv_output_dim(self, input_dim, num_convs, stride, kernel_size, num_filters):\n",
    "        output_dim = input_dim\n",
    "        for _ in range(num_convs):\n",
    "            output_dim = ((output_dim - kernel_size + 2 * (kernel_size // 2)) // stride + 1) // 2  # Pooling divides size by 2\n",
    "        return output_dim * output_dim * output_dim * num_filters\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, label_mapping, num_epochs=10, patience=5, device='cuda'):\n",
    "    model.to(device)\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    val_losses = []  # To store validation losses for monitoring\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0  # Counter for epochs with no improvement\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        train_epoch_losses = []\n",
    "        \n",
    "        for images, labels, _, _ in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Train'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_epoch_losses.append(loss.item())\n",
    "\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted_indices == labels).sum().item()\n",
    "\n",
    "        train_avg_loss = sum(train_epoch_losses) / len(train_epoch_losses)\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_avg_loss:.4f} - Train Accuracy: {train_accuracy:.2f}%')\n",
    "        \n",
    "        # Validation phase at the end of each epoch\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_epoch_losses = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels, _, _ in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validate'):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                val_epoch_losses.append(val_loss.item())\n",
    "                _, predicted_indices = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted_indices == labels).sum().item()\n",
    "\n",
    "        val_avg_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n",
    "        val_losses.append(val_avg_loss)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f'Epoch {epoch+1}: Validation Loss: {val_avg_loss:.4f} - Validation Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_avg_loss < best_val_loss:\n",
    "            best_val_loss = val_avg_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs.')\n",
    "            break  # Break out of the loop if no improvement for 'patience' consecutive epochs\n",
    "\n",
    "    return train_accuracies, val_accuracies, val_losses\n",
    "\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, label_mapping, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths, subjects in tqdm(test_loader, desc='Testing'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted_indices == labels).sum().item()\n",
    "            predicted_labels = [label_mapping[code] for code in predicted_indices.cpu().numpy()]\n",
    "\n",
    "            for label, pred, path, subject in zip(labels.cpu().numpy(), predicted_labels, paths, subjects):\n",
    "                test_results.append({\n",
    "                    'Subject': subject,\n",
    "                    'Path': path,\n",
    "                    'Actual Label': label_mapping[label.item()],\n",
    "                    'Prediction': pred,\n",
    "                    'Type': 'Test'\n",
    "                })\n",
    "    accuracy = 100 * correct / total\n",
    "    return test_results, accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def run_experiment(df, config):\n",
    "    \"\"\"Run the experiment with the given configuration on the preprocessed DataFrame.\"\"\"\n",
    "    train_dataset, val_dataset, test_dataset = load_datasets(df, config['image_type'])\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Baseline3DCNN(num_classes=config['num_classes'], init_filters=config['init_filters'],\n",
    "                          kernel_size=config['kernel_size'], stride=config['stride'], num_fc_units=config['num_fc_units']).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training and validation\n",
    "    train_accuracies, val_accuracies, val_losses = train_and_validate(model, train_loader, val_loader, criterion, optimizer, label_mapping, config['num_epochs'], config['patience'], device)\n",
    "    test_results, test_accuracy = test_model(model, test_loader, label_mapping, device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "  # Save detailed results to Excel\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = os.path.join('reports', f'{formatted_time}_Experiment.xlsx')\n",
    "    \n",
    "    summary_data = {\n",
    "        'Phase': ['Training', 'Validation', 'Testing'],\n",
    "        'Accuracy': [train_accuracies[-1], val_accuracies[-1], test_accuracy]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    all_results = pd.DataFrame(test_results)\n",
    "    config_df = pd.DataFrame([config])\n",
    "    \n",
    "    with pd.ExcelWriter(filename) as writer:\n",
    "        config_df.to_excel(writer, sheet_name='Configuration')\n",
    "        all_results.to_excel(writer, sheet_name='Results')\n",
    "        summary_df.to_excel(writer, sheet_name='Summary')\n",
    "\n",
    "    # Append a summary of this experiment to the cumulative RESULTS.xlsx file\n",
    "    results_file = os.path.join('reports', 'RESULTS.xlsx')\n",
    "    experiment_summary = {**config, **{'Training Accuracy': train_accuracies[-1], 'Validation Accuracy': val_accuracies[-1], 'Test Accuracy': test_accuracy, 'DATETIME': formatted_time}}\n",
    "    summary_row = pd.DataFrame([experiment_summary])\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        with pd.ExcelWriter(results_file, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "            existing_df = pd.read_excel(results_file)\n",
    "            # Reindex the existing DataFrame to ensure all columns are aligned and add new columns if necessary\n",
    "            combined_df = pd.concat([existing_df, summary_row], ignore_index=True)\n",
    "            combined_df = combined_df.reindex(columns=(existing_df.columns.tolist() + [col for col in summary_row.columns if col not in existing_df.columns]))\n",
    "            combined_df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "    else:\n",
    "        summary_row.to_excel(results_file, index=False)\n",
    "\n",
    "    return filename, train_accuracies[-1], val_accuracies[-1], test_accuracy\n",
    "\n",
    "# Example configuration and use case\n",
    "config = {\n",
    "    'num_classes': 2,\n",
    "    'init_filters': 128,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'num_fc_units': 128,\n",
    "    'optimizer': 'Adam',\n",
    "    'loss_criterion': 'BCEWithLogitsLoss',\n",
    "    'num_epochs': 2,\n",
    "    'batch_size': 1,\n",
    "    'patience': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on image type: MRI_PET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Train: 100%|██████████| 76/76 [00:10<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 6.5417 - Train Accuracy: 46.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Validate: 100%|██████████| 26/26 [00:03<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Loss: 0.6931 - Validation Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Train: 100%|██████████| 76/76 [00:10<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.6955 - Train Accuracy: 46.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Validate: 100%|██████████| 26/26 [00:03<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Loss: 0.6932 - Validation Accuracy: 50.00%\n",
      "No improvement in validation loss for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 26/26 [00:03<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on image type: MRI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Train: 100%|██████████| 76/76 [00:10<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.2896 - Train Accuracy: 46.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Validate: 100%|██████████| 26/26 [00:03<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Loss: 0.6934 - Validation Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Train: 100%|██████████| 76/76 [00:10<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.6940 - Train Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Validate: 100%|██████████| 26/26 [00:03<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Loss: 0.6933 - Validation Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 26/26 [00:03<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on image type: PET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Train: 100%|██████████| 76/76 [00:10<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 18.7611 - Train Accuracy: 60.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Validate: 100%|██████████| 26/26 [00:03<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Loss: 0.6937 - Validation Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Train: 100%|██████████| 76/76 [00:10<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.6952 - Train Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Validate: 100%|██████████| 26/26 [00:03<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Loss: 0.6932 - Validation Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 26/26 [00:03<00:00,  7.86it/s]\n"
     ]
    }
   ],
   "source": [
    "image_types = ['MRI_PET', 'MRI', 'PET']\n",
    "results = []\n",
    "for image_type in image_types:\n",
    "    config['image_type'] = image_type\n",
    "    print(f\"Working on image type: {image_type}\")\n",
    "    result = run_experiment(df, config)\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def append_to_excel(results_file, new_data):\n",
    "    \"\"\"Appends a DataFrame to an existing Excel file or creates a new one if it doesn't exist.\"\"\"\n",
    "    if os.path.exists(results_file):\n",
    "        book = load_workbook(results_file)\n",
    "        with pd.ExcelWriter(results_file, engine='openpyxl') as writer:\n",
    "            writer.book = book\n",
    "            writer.sheets = {ws.title: ws for ws in book.worksheets}\n",
    "            existing_data = pd.read_excel(results_file)\n",
    "            combined_data = pd.concat([existing_data, new_data], ignore_index=True, sort=False)\n",
    "            combined_data.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "    else:\n",
    "        new_data.to_excel(results_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "def run_experiment(df, config):\n",
    "    \"\"\"Run the experiment with the given configuration on the preprocessed DataFrame.\"\"\"\n",
    "    train_dataset, val_dataset, test_dataset = load_datasets(df, config['image_type'])\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # Add the image type to the configuration output in Excel\n",
    "    config['Image Type'] = config['image_type'] \n",
    "    \n",
    "    # Initialize model and training components\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Baseline3DCNN(num_classes=config['num_classes'], init_filters=config['init_filters'],\n",
    "                          kernel_size=config['kernel_size'], stride=config['stride'], num_fc_units=config['num_fc_units']).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training and validation\n",
    "    train_results, train_accuracies = train_model(model, train_loader, criterion, optimizer, label_mapping, config['num_epochs'], device)\n",
    "    validate_results, val_accuracy = validate_model(model, val_loader, criterion, label_mapping, device)\n",
    "    test_results, test_accuracy = test_model(model, test_loader, label_mapping, device)\n",
    "    \n",
    "    # Save detailed results to Excel\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = os.path.join('reports', f'{formatted_time}_Experiment.xlsx')\n",
    "    \n",
    "    summary_data = {\n",
    "        'Phase': ['Training', 'Validation', 'Testing'],\n",
    "        'Accuracy': [train_accuracies[-1], val_accuracy, test_accuracy]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    all_results = pd.DataFrame(train_results + validate_results + test_results)\n",
    "    config_df = pd.DataFrame([config])\n",
    "    \n",
    "    with pd.ExcelWriter(filename) as writer:\n",
    "        config_df.to_excel(writer, sheet_name='Configuration')\n",
    "        all_results.to_excel(writer, sheet_name='Results')\n",
    "        summary_df.to_excel(writer, sheet_name='Summary')\n",
    "\n",
    "# Append a summary of this experiment to the cumulative RESULTS.xlsx file\n",
    "    results_file = os.path.join('reports', 'RESULTS.xlsx')\n",
    "    experiment_summary = {**config, **{\n",
    "        'Training Accuracy': train_accuracies[-1],\n",
    "        'Validation Accuracy': val_accuracy,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'DATETIME': formatted_time\n",
    "    }}\n",
    "    new_row = pd.DataFrame([experiment_summary])\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        # Load the existing Excel file and sheet\n",
    "        book = load_workbook(results_file)\n",
    "        with pd.ExcelWriter(results_file, engine='openpyxl') as writer:\n",
    "            writer.book = book\n",
    "            writer.sheets = {ws.title: ws for ws in book.worksheets}\n",
    "\n",
    "            # Read existing data\n",
    "            existing_data = pd.read_excel(results_file)\n",
    "            \n",
    "            # Combine new row with existing DataFrame, aligning on columns\n",
    "            combined_data = pd.concat([existing_data, new_row], ignore_index=True, sort=False)\n",
    "\n",
    "            # Write updated DataFrame to Excel, replacing old data\n",
    "            combined_data.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "            writer.save()\n",
    "    else:\n",
    "        # If the file does not exist, simply write the new DataFrame\n",
    "        new_row.to_excel(results_file, index=False)\n",
    "\n",
    "    return filename, train_accuracies[-1], val_accuracy, test_accuracy\n",
    "\n",
    "# Example of how to call run_experiment\n",
    "# Assuming 'df' has been preprocessed already\n",
    "config = {\n",
    "    'num_classes': 2,\n",
    "    'init_filters': 128,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'num_fc_units': 128,\n",
    "    'optimizer': 'Adam',\n",
    "    'loss_criterion': 'BCEWithLogitsLoss',\n",
    "    'num_epochs': 1,\n",
    "    'batch_size': 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "def save_experiment_results(config, train_results, train_accuracies, validate_results, val_accuracy, test_results, test_accuracy):\n",
    "    \"\"\"Saves detailed results of an experiment to an Excel file and appends a summary to a cumulative results file.\"\"\"\n",
    "    # Setup file paths and names\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    detailed_filename = os.path.join('reports', f'{formatted_time}_Experiment.xlsx')\n",
    "    summary_results_file = os.path.join('reports', 'RESULTS.xlsx')\n",
    "\n",
    "    # Prepare dataframes for detailed results\n",
    "    summary_data = {'Phase': ['Training', 'Validation', 'Testing'], 'Accuracy': [train_accuracies[-1], val_accuracy, test_accuracy]}\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    all_results = pd.DataFrame(train_results + validate_results + test_results)\n",
    "    config_df = pd.DataFrame([config])\n",
    "    \n",
    "    # Write detailed results to Excel\n",
    "    with pd.ExcelWriter(detailed_filename) as writer:\n",
    "        config_df.to_excel(writer, sheet_name='Configuration')\n",
    "        all_results.to_excel(writer, sheet_name='Results')\n",
    "        summary_df.to_excel(writer, sheet_name='Summary')\n",
    "\n",
    "    # Prepare summary row for cumulative results\n",
    "    experiment_summary = {**config, **{\n",
    "        'Training Accuracy': train_accuracies[-1],\n",
    "        'Validation Accuracy': val_accuracy,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'DATETIME': formatted_time\n",
    "    }}\n",
    "    summary_row = pd.DataFrame([experiment_summary])\n",
    "\n",
    "    # Append summary row to the cumulative RESULTS.xlsx file\n",
    "    append_to_excel(summary_results_file, summary_row)\n",
    "\n",
    "    return detailed_filename\n",
    "\n",
    "def append_to_excel(results_file, new_data):\n",
    "    \"\"\"Appends a DataFrame to an existing Excel file or creates a new one if it doesn't exist.\"\"\"\n",
    "    if os.path.exists(results_file):\n",
    "        book = load_workbook(results_file)\n",
    "        with pd.ExcelWriter(results_file, engine='openpyxl') as writer:\n",
    "            writer.book = book\n",
    "            writer.sheets = {ws.title: ws for ws in book.worksheets}\n",
    "            existing_data = pd.read_excel(results_file)\n",
    "            combined_data = pd.concat([existing_data, new_data], ignore_index=True, sort=False)\n",
    "            combined_data.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "    else:\n",
    "        new_data.to_excel(results_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(df, config):\n",
    "    \"\"\"Run the experiment with the given configuration on the preprocessed DataFrame.\"\"\"\n",
    "    train_dataset, val_dataset, test_dataset = load_datasets(df, config['image_type'])\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = Baseline3DCNN(num_classes=config['num_classes'], init_filters=config['init_filters'],\n",
    "                          kernel_size=config['kernel_size'], stride=config['stride'], num_fc_units=config['num_fc_units']).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training and validation\n",
    "    train_results, train_accuracies = train_model(model, train_loader, criterion, optimizer, config['num_epochs'], device=device)\n",
    "    validate_results, val_accuracy = validate_model(model, val_loader, criterion, device=device)\n",
    "    test_results, test_accuracy = test_model(model, test_loader, device=device)\n",
    "    \n",
    "    # Save results to Excel\n",
    "    filename = save_experiment_results(config, train_results, train_accuracies, validate_results, val_accuracy, test_results, test_accuracy)\n",
    "\n",
    "    return filename, train_accuracies[-1], val_accuracy, test_accuracy\n",
    "\n",
    "config = {\n",
    "    'num_classes': 2,\n",
    "    'init_filters': 128,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'num_fc_units': 128,\n",
    "    'optimizer': 'Adam',\n",
    "    'loss_criterion': 'BCEWithLogitsLoss',\n",
    "    'num_epochs': 1,\n",
    "    'batch_size': 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
